[{"content":"Code Repository: github.com/umutkavakli/online-softmax\nAs sequences get longer in Transformer models, the standard approach to processing data becomes incredibly expensive and often exceeds GPU memory limits. Instead of processing the entire sequence as a single large block, we can split it into smaller chunks and merge them incrementally. This method ensures that sequence generation remains efficient and produces exact results while keeping memory usage under control.\nThe softmax function is an crucial part of modern deep learning. It is especially important for the attention mechanism used in Transformer models. In self-attention, softmax converts raw similarity scores into a probability distribution. This distribution tells the model how much focus to put on each token in a sequence. We usually define scaled dot product attention like this:\n$$ \\text{Attention(Q, K, V)} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} \\right) V $$\nWhen we calculate the dot product of queries and keys, we get an $N \\times N$ a sequence of length $N$. This creates a major problem because the time and memory needed grow at a quadratic rate $O(N^2)$:\nFor very long sequences, a single GPU cannot store the entire attention matrix. The Ring Attention paper highlights just how extreme this memory demand can be:\nTo put the memory demand in perspective, even when dealing with a batch size of 1, processing 100 million tokens requires over 1000 GB of memory for a modest model with a hidden size of 1024.\nTo solve this, we can split the attention matrix into smaller blocks and run calculations on those blocks individually:\nProcessing in blocks is a great idea, but the standard softmax function makes it difficult because it needs the sum of every element in the sequence to calculate the denominator.\nNaive Softmax The standard softmax formula is defined as:\n$$ \\text{Softmax}(x_i) = \\frac{\\exp(x_i)}{\\sum_{j=1}^{N} \\exp(x_j)} $$\nWe can write a simple version of this in PyTorch:\nimport torch def naive_softmax(x: torch.Tensor) -\u0026gt; torch.Tensor: return x.exp() / x.exp().sum() If we compare this naive function to the official PyTorch version, the results look identical for small numbers:\nx = torch.randn(8) reference = torch.softmax(x, dim=-1) naive = naive_softmax(x) print(f\u0026#34;Torch: {reference}\u0026#34;) print(f\u0026#34;Naive: {naive}\u0026#34;) print(f\u0026#34;allclose: {torch.allclose(reference, naive)}\u0026#34;) # Output Torch: tensor([0.1085, 0.0730, 0.3312, 0.2468, 0.1182, 0.0637, 0.0182, 0.0404]) Naive: tensor([0.1085, 0.0730, 0.3312, 0.2468, 0.1182, 0.0637, 0.0182, 0.0404]) allclose: True However, this naive version is not stable. When the input values are large, the exponential function grows too fast and causes a numerical overflow. This results in \u0026ldquo;nan\u0026rdquo; errors in naive softmax:\nnaive_softmax(x * 1000) # Output tensor([0., 0., nan, nan, 0., 0., 0., 0.]) Safe Softmax Softmax has the shift invariance property. This means that if we add or subtract the same constant from every input, the output stays the same. We can use this to keep our numbers small and manageable. By subtracting the maximum value from the entire vector, we ensure that the largest exponent is exactly zero.\n$$ \\begin{align*} \\text{SafeSoftmax}(x_i) \u0026amp;= \\frac{\\exp(x_i)}{\\sum_{j=1}^{N} \\exp(x_j)} \\cdot \\frac{\\exp(- \\max_{k=1}^{N} x_k)}{\\exp(- \\max_{k=1}^{N} x_k)} \\\\ \\\\ \u0026amp;= \\frac{\\exp(x_i - \\max_{k=1}^{N} x_k)}{\\sum_{j=1}^{N} \\exp(x_j - \\max_{k=1}^{N} x_k)} \\end{align*} $$\nBy doing this, every exponent is less than or equal to zero, which completely prevents overflow. Here is how we implement it:\ndef safe_softmax(x: torch.Tensor) -\u0026gt; torch.Tensor: m = x.max() return (x - m).exp() / (x - m).exp().sum() reference = torch.softmax(x, dim=-1) safe = safe_softmax(x) print(f\u0026#34;Torch: {reference}\u0026#34;) print(f\u0026#34;Safe: {safe}\u0026#34;) print(f\u0026#34;allclose: {torch.allclose(reference, safe)}\u0026#34;) # Output Torch: tensor([0.1085, 0.0730, 0.3312, 0.2468, 0.1182, 0.0637, 0.0182, 0.0404]) Safe: tensor([0.1085, 0.0730, 0.3312, 0.2468, 0.1182, 0.0637, 0.0182, 0.0404]) allclose: True This version is also stable for large inputs:\nsafe_softmax(x * 1000) # Output tensor([0., 0., 1., 0., 0., 0., 0., 0.]) Online Softmax In the attention mechanism, we apply softmax row by row. To calculate the denominator for a single query, we need to access every single key vector in that row. This global dependency is exactly what makes it hard to process the matrix in chunks:\nOnline softmax allows us to calculate the result step by step. If we track the local maximum $m$ and the local sum $d$ for each small block, we can update them as we move through the sequence. When we receive a new element $x_S$, we update our statistics like this:\n$$ \\begin{align*} m_S \u0026amp;\\leftarrow \\max(m_{S-1}, x_S) \\\\ \\\\ d_S \u0026amp;\\leftarrow d_{S-1} \\cdot\\exp(m_{S-1} - m_S) + \\exp(x_{S} - m_S) \\end{align*} $$\nClick for the proof $$ \\begin{align*} m_S \u0026amp;\\leftarrow \\max(m_{S-1}, x_S) \\\\ \u0026amp;= \\max(\\max_{k=1}^{S-1} x_k, x_S) \\\\ \u0026amp;= \\max_{k=1}^{S} x_k \\\\ \\\\ d_S \u0026amp;\\leftarrow d_{S-1} \\cdot\\exp(m_{S-1} - m_S) + \\exp(x_{S} - m_S) \\\\ \u0026amp;= \\left(\\sum_{j=1}^{S-1} \\exp(x_j - {\\color{red} \\cancel{m_{S-1}}}) \\right) \\cdot \\exp({\\color{red} \\cancel{m_{S-1}}} - m_S) + \\exp(x_{S} - m_S) \\\\ \u0026amp;= \\left(\\sum_{j=1}^{S-1} \\exp(x_j - m_{S}) \\right) + \\exp(x_{S} - m_S) \\\\ \u0026amp;= \\sum_{j=1}^{S} \\exp(x_j - m_{S}) \\end{align*} $$\nThis logic allows us to merge several local results into one global distribution. If we have $B$ blocks, each with its own max value $m_i$ and sum $d_i$, we can combine them by rescaling the local probabilities. This approach perfectly matches the standard softmax output:\n$$ \\begin{align*} \\text{OnlineSoftmax}(p_i, m_i, d_i) \u0026amp;= \\frac{p_i \\cdot d_i \\cdot \\exp(m_i -\\max_{k=1}^{B})}{\\sum_{j=1}^B d_j \\cdot \\exp(m_j - \\max_{k=1}^{B})} \\\\ \\end{align*} $$\nClick for the proof $$ \\begin{align*} \\text{OnlineSoftmax}(p_i, m_i, d_i) \u0026amp;= \\frac{p_i \\cdot d_i \\cdot \\exp(m_i -\\max_{k=1}^{B})}{\\sum_{j=1}^B d_j \\cdot \\exp(m_j - \\max_{k=1}^{B})} \\\\ \\\\ \u0026amp;= \\frac{\\frac{\\exp(x_i - m_i)}{{\\color{red} \\cancel{d_i}}} \\cdot {\\color{red} \\cancel{d_i}} \\cdot \\exp(m_i -\\max_{k=1}^{B})}{\\sum_{j=1}^B d_j \\cdot \\exp(m_j - \\max_{k=1}^{B})} \\\\ \\\\ \u0026amp;= \\frac{\\exp(x_i - {\\color{red} \\cancel{m_i}}) \\cdot \\exp({\\color{red} \\cancel{m_i}} -\\max_{k=1}^{B})}{\\sum_{j=1}^B d_j \\cdot \\exp(m_j - \\max_{k=1}^{B})} \\\\ \\\\ \u0026amp;= \\frac{\\exp(x_i -\\max_{k=1}^{B})}{\\sum_{j=1}^B d_j \\cdot \\exp(m_j - \\max_{k=1}^{B})} \\\\ \\\\ \u0026amp;= \\frac{\\exp(x_i -\\max_{k=1}^{B})}{\\sum_{j=1}^B \\left[\\sum_{l=1}^{S_j} \\exp(x_l - {\\color{red} \\cancel{m_i}}) \\right] \\cdot \\exp({\\color{red} \\cancel{m_i}} - \\max_{k=1}^{B})} \\\\ \\\\ \u0026amp;= \\frac{\\exp(x_i -\\max_{k=1}^{B})}{\\sum_{j=1}^N \\exp(x_j - \\max_{k=1}^{B})} \\quad\\quad\\quad \\left(N = \\sum_{j=1}^{B} S_j \\right)\\\\ \\end{align*} $$\nWe first need a modified version of safe softmax that returns the local maximum and the denominator sum:\ndef safe_softmax2(x: torch.Tensor) -\u0026gt; torch.Tensor: m = x.max() a = (x - m).exp() # subtract maximum value d = a.sum() # normalization factor return a / d, m, d Next, we create the online softmax function. It takes these local blocks and merges them by finding a global maximum and adjusting the sums accordingly:\ndef online_softmax( *blocks: Tuple[torch.Tensor, torch.Tensor, torch.Tensor] ) -\u0026gt; torch.Tensor: p_blocks, m_blocks, d_blocks = zip(*blocks) # get global maximum m_max = torch.stack(m_blocks).max() # compute global normalizer d_total = sum( d * torch.exp(m - m_max) for d, m in zip(d_blocks, m_blocks) ) return torch.cat( [ p * d * torch.exp(m - m_max) / d_total for p, m, d in zip(p_blocks, m_blocks, d_blocks) ] ) Finally, we can verify that splitting the input into four chunks and processing them with our online softmax function gives the correct result:\nblocks = [] for chunk in list(x.chunk(4)): blocks.append(safe_softmax2(chunk)) # compute p, m, d values per chunk online = online_softmax(*blocks) print(f\u0026#34;Torch: {reference}\u0026#34;) print(f\u0026#34;Online: {online}\u0026#34;) print(f\u0026#34;allclose: {torch.allclose(reference, online)}\u0026#34;) # Output Torch: tensor([0.1085, 0.0730, 0.3312, 0.2468, 0.1182, 0.0637, 0.0182, 0.0404]) Online: tensor([0.1085, 0.0730, 0.3312, 0.2468, 0.1182, 0.0637, 0.0182, 0.0404]) allclose: True Optimized Online Softmax We can make this even more efficient. Instead of tracking the max value and the sum separately, we can combine them into a single value called the logarithm of the sum of exponentiated values or Log Sum Exp (LSE). We define this as $\\text{lse}_i = m_i + \\log(d_i)$. This simplifies our global formula significantly:\n$$ \\begin{align*} \\text{OnlineSoftmax}(p_i, \\text{lse}_i) \u0026amp;= p_i \\cdot \\frac{\\exp(\\text{lse}_{i})}{\\sum_{j=1}^{B} \\exp(\\text{lse}_j)} \\end{align*} $$\nClick for the proof $$ \\begin{align*} \\text{OnlineSoftmax}(p_i, \\text{lse}_i) \u0026amp;= p_i \\cdot \\frac{\\exp(\\text{lse}_{i})}{\\sum_{j=1}^{B} \\exp(\\text{lse}_j)} \\\\ \\\\ \u0026amp;= \\frac{\\exp(x_i - {\\color{orange} \\cancel{m_i}})}{{\\color{red} \\cancel{d_i}}} \\cdot \\frac{\\exp({\\color{orange} \\cancel{m_i}}) \\cdot {\\color{red} \\cancel{d_i}}}{\\sum_{j=1}^{B} \\exp(m_j) \\cdot d_j} \\\\ \\\\ \u0026amp;= \\frac{\\exp(x_i)}{\\sum_{j=1}^{B} \\exp(m_j) \\cdot \\left[\\sum_{l=1}^{S_j} \\exp(x_l - m_j)\\right]} \\\\ \\\\ \u0026amp;= \\frac{\\exp(x_i)}{\\sum_{j=1}^{B} \\sum_{l=1}^{S_j} \\exp(x_l - {\\color{red} \\cancel{m_j}} + {\\color{red} \\cancel{m_j}} )}\\\\ \\\\ \u0026amp;= \\frac{\\exp(x_i)}{\\sum_{j=1}^{N} \\exp(x_j)} \\quad\\quad\\quad \\left(N = \\sum_{j=1}^{B} S_j \\right) \\end{align*} $$\nTo use this approach, we slightly modify the safe_softmax function so that it also returns the $\\text{lse}$ value instead of $m$ and $d$ separately:\ndef safe_softmax3(x: torch.Tensor) -\u0026gt; torch.Tensor: m = x.max() a = (x - m).exp() b = a.sum() lse = m + torch.log(b) return a / b, lse # return local softmax and lse outputs The new online softmax function now uses these LSE weights to rescale the local probabilities before joining them together:\ndef online_softmax2( p_blocks = List[torch.Tensor], lse_blocks = List[torch.Tensor] ) -\u0026gt; torch.Tensor: weights = torch.exp(torch.stack(lse_blocks)) return torch.cat([ p * w for p, w in zip(p_blocks, weights) ]) / weights.sum() This version still gives us the exact same output but with a cleaner mathematical structure:\np_blocks = [] lse_blocks = [] for chunk in list(x.chunk(4)): p, lse = safe_softmax3(chunk) # compute p, lse values per chunk p_blocks.append(p) lse_blocks.append(lse) online2 = online_softmax2(p_blocks, lse_blocks) print(f\u0026#34;Torch: {reference}\u0026#34;) print(f\u0026#34;Online2: {online2}\u0026#34;) print(f\u0026#34;allclose: {torch.allclose(reference, online2)}\u0026#34;) # Output Torch: tensor([0.1085, 0.0730, 0.3312, 0.2468, 0.1182, 0.0637, 0.0182, 0.0404]) Online2: tensor([0.1085, 0.0730, 0.3312, 0.2468, 0.1182, 0.0637, 0.0182, 0.0404]) allclose: True Practical Implementation Although previous generalized version is really nice to compute global softmax by considering multiple local softmax outputs, Ring/Striped attention uses next key/value pairs to update softmax output each time. As the data moves between GPUs, the hardware calculates attention for the local block and then passes it along:\nBecause we are only dealing with two blocks, we can simplify the formula even further. Using the property of $\\frac{1}{1 + B/A} = \\frac{A}{A + B}$: we can reduce the number of times we use the exponential function:\n$$ \\begin{align*} \\text{OnlineSoftmax}(p_i, \\text{lse}_i) \u0026amp;= p_i \\cdot \\frac{1}{1 + \\exp(\\text{lse}_{1-i} - \\text{lse}_i)} \\quad\\quad\\quad \\left(i\\in \\{0,1\\} \\right) \\\\ \\end{align*} $$\nClick for the proof $$ \\begin{align*} \\text{OnlineSoftmax}(p_i, \\text{lse}_i) \u0026amp;= p_i \\cdot \\frac{\\exp(\\text{lse}_{i})}{ \\exp(\\text{lse}_i) + \\exp(\\text{lse}_{1-i})} \\quad\\quad\\quad \\left(i\\in \\{0,1\\} \\right) \\\\ \\\\ \u0026amp;= p_i \\cdot \\frac{1}{1 + \\exp(\\text{lse}_{1-i}) / \\exp(\\text{lse}_i)} \\quad\\quad \\left(\\frac{1}{1 + B/A} \\right) \\\\ \\\\ \u0026amp;= p_i \\cdot \\frac{1}{1 + \\exp(\\text{lse}_{1-i} - \\text{lse}_i)} \\end{align*} $$\nSince our goal is to iteratively compute updated softmax value for given new $p$ and $\\text{lse}$, we also need to compute $\\text{lse}_{\\text{new}}$. Since $\\text{lse}_{i} = m_i + \\log(d_i)$, we can find $\\text{lse}_{\\text{new}}$ as follows:\n$$ \\begin{align*} \\text{lse}_{\\text{new}} \u0026amp;= \\text{lse}_0 + \\log\\left(1 + \\exp(\\text{lse}_1 - \\exp(\\text{lse}_0)) \\right) \\end{align*} $$\nClick for the proof $$ \\begin{align*} \\text{lse}_{\\text{new}} \u0026amp;= \\log(\\exp(\\text{lse}_0) + \\exp(\\text{lse}_1)) \\\\ \\\\ \u0026amp;= \\log\\left(\\exp(\\text{lse}_0) \\left[1 + \\frac{ \\exp(\\text{lse}_1)}{ \\exp(\\text{lse}_0)} \\right]\\right) \\\\ \\\\ \u0026amp;= \\text{lse}_0 + \\log\\left(1 + \\exp(\\text{lse}_1 - \\exp(\\text{lse}_0)) \\right) \\end{align*} $$\nThe implementation of this iterative update looks like this:\ndef online_softmax3( p0: torch.Tensor, lse0: torch.Tensor, p1: torch.Tensor, lse1: torch.Tensor ) -\u0026gt; tuple[torch.Tensor, torch.Tensor]: out0 = p0 / (1 + torch.exp(lse1 - lse0)) out1 = p1 / (1 + torch.exp(lse0 - lse1)) new_lse = lse0 + torch.log(1 + torch.exp(lse1 - lse0)) return torch.cat([out0, out1]), new_lse We can now compute the global softmax result by looping through the chunks one by one:\np, lse = p_blocks[0], lse_blocks[0] for p_new, lse_new in zip(p_blocks[1:], lse_blocks[1:]): p, lse = online_softmax3(p, lse, p_new, lse_new) print(f\u0026#34;Torch: {reference}\u0026#34;) print(f\u0026#34;Online3: {p}\u0026#34;) print(f\u0026#34;allclose: {torch.allclose(reference, p)}\u0026#34;) # Output Torch: tensor([0.1085, 0.0730, 0.3312, 0.2468, 0.1182, 0.0637, 0.0182, 0.0404]) Online: tensor([0.1085, 0.0730, 0.3312, 0.2468, 0.1182, 0.0637, 0.0182, 0.0404]) allclose: True If you look at the source code for Ring Flash Attention in PyTorch, you will see a very similar pattern. They use a function to update the output and the LSE values iteratively, which allows them to handle massive sequences across multiple GPUs:\ndef _update_out_and_lse( out: torch.Tensor, lse: torch.Tensor, block_out: torch.Tensor, block_lse: torch.Tensor, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: block_out = block_out.to(torch.float32) block_lse = block_lse.transpose(-2, -1).unsqueeze(dim=-1) new_lse = lse + torch.log(1 + torch.exp(block_lse - lse)) out = torch.exp(lse - new_lse) * out + torch.exp(block_lse - new_lse) * block_out lse = new_lse return out, lse References [1] Maxim Milakov, et al. Online normalizer calculation for softmax. arXiv:1805.02867. 2018\n[2] Tri Dao, et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. arXiv:2205.14135. 2022.\n[3] Hao Liu, et al. Blockwise Parallel Transformer for Large Context Models. arXiv:2305.19370. 2023.\n[4] Hao Liu, et al. Ring Attention with Blockwise Transformers for Near-Infinite Context. arXiv:2310.01889. 2023.\n[5] William Brandon, et al. Striped Attention: Faster Ring Attention for Causal Transformers. arXiv:2311.09431. 2023.\n[6] Christian Mills. GPU Mode Blog Post. 2024.\n[7] Kilian Haefeli, et al. Blog Post. 2024.\n","permalink":"https://umutkavakli.github.io/posts/2026-01-10-online-softmax/","summary":"\u003cp\u003e\u003cstrong\u003eCode Repository:\u003c/strong\u003e \u003ca href=\"https://github.com/umutkavakli/online-softmax\"\u003egithub.com/umutkavakli/online-softmax\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAs sequences get longer in Transformer models, the standard approach to processing data becomes incredibly expensive and often exceeds GPU memory limits. Instead of processing the entire sequence as a single large block, we can split it into smaller chunks and merge them incrementally. This method ensures that sequence generation remains efficient and produces exact results while keeping memory usage under control.\u003c/p\u003e\n\u003cfigure class=\"align-center \"\u003e\n    \u003cimg loading=\"lazy\" src=\"images/onlinesoftmax.png#center\"/\u003e \n\u003c/figure\u003e\n\n\u003cp\u003eThe \u003ca href=\"https://en.wikipedia.org/wiki/Softmax_function\"\u003esoftmax\u003c/a\u003e function is an crucial part of modern deep learning. It is especially important for the attention mechanism used in Transformer models. In self-attention, softmax converts raw similarity scores into a probability distribution. This distribution tells the model how much focus to put on each token in a sequence. We usually define scaled dot product attention like this:\u003c/p\u003e","title":"Online Softmax in Attention Mechanism"},{"content":"TL;DR In this long and technical blog, I explained:\nForward propagation: How inputs flow through the network layer by layer (using matrix operations) to generate predictions.\nComputation Graph: How simple scalar examples help visualize backpropagation and build intuition before scaling up.\nBackward propagation: How errors flow backward (again with matrix operations) to compute gradients and update weights.\nBackpropagation appears quite straightforward when working with scalars or even simple vectors. However, once we step into the world of matrices, things quickly become more complex and difficult to follow. There are extra details and notations that make it less intuitive.\nPersonally, although I managed to understand this concept while preparing for deep learning exams, I usually lose my intuition a few months later. Reviewing it again requires extra effort to bring everything back together. That\u0026rsquo;s why I decided to write this technical blog post, both as a reminder for myself and as a guide for anyone trying to understand backpropagation in matrix form.\nForward Propagation When training a deep learning model, the first step is to compute a linear combination of input features. Given an input vector $x \\in \\mathbb{R}^M$ with $M$ features and a corresponding weight vector $w \\in \\mathbb{R}^M$, we calculate a weighted sum and add a bias term $b \\in \\mathbb{R}^1$ to produce a single output $z$:\n$$ z = w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\dots + w_M \\cdot x_M + b $$\nThis is simply the dot product between $w$ and $x$ with an added bias term:\n$$ z = w^T x + b $$\nTo visualize this computation, we can represent it in matrix form:\n$$ \\begin{bmatrix} z \\end{bmatrix} = \\begin{bmatrix} w_1 \u0026amp; w_2 \u0026amp; \\cdots \u0026amp; w_M \\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_M \\end{bmatrix} + \\begin{bmatrix} b \\end{bmatrix} $$\nor in image representation:\nWe can simplify our notation by incorporating the bias term directly into the weight vector. This involves adding the bias $b$ as the first element of the weight vector and including a constant input $x_0 = 1$:\n$$ \\begin{bmatrix} z \\end{bmatrix} = \\begin{bmatrix} b \u0026amp; w_1 \u0026amp; w_2 \u0026amp; \\cdots \u0026amp; w_M \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_M \\end{bmatrix} $$\nFor notational clarity, we\u0026rsquo;ll denote the bias term as $w_0 = b$ and the constant input as $x_0 = 1$. Under this convention, both vectors have dimension $M+1$: $w \\in \\mathbb{R}^{M+1}$ and $x \\in \\mathbb{R}^{M+1}$. Our equation becomes:\n$$ z = w^T x $$\nor in matrix form:\n$$ \\begin{bmatrix} z \\end{bmatrix} = \\begin{bmatrix} w_0 \u0026amp; w_1 \u0026amp; w_2 \u0026amp; \\cdots \u0026amp; w_M \\end{bmatrix} \\cdot \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_M \\end{bmatrix} $$\nor in image representation:\nAdding Non-Linearity The linear combination alone is insufficient for learning complex patterns. To add non-linearity, we apply an activation function $\\sigma(\\cdot)$ to the output $z$.\n$$ a = \\sigma(z) = \\sigma(w^Tx) $$\nAlthough $\\sigma$ often denotes the Sigmoid function, here it represents a general activation function. For this explanation, we\u0026rsquo;ll use the ReLU activation function in hidden layers due to its simplicity, computational efficiency, resistance to vanishing gradients, and widespread popularity:\n$$ a = \\sigma(z) = \\max(0, z) $$\nwe can show this with visual representation:\nMulti-class Classification In practice, deep learning models often solve multi-class problems where we need to predict one of $C$ possible classes. This requires the output $a$ to be $C$-dimensional vector, with each dimension representing the score for a particular class. The predicted class corresponds to the entry with the highest activation value.\nTo generate $C$ outputs, we need $C$ distinct weight vectors, each of dimension $(M+1)$. To achieve this, we need a separate weight vector for each class. Stacking them gives a weight matrix $W \\in \\mathbb{R}^{C \\times (M+1)}$:\n$$ \\underbrace{a}_{[C \\times 1]} = \\sigma \\left( \\underbrace{W}_{[C \\times (M+1)]} \\quad \\underbrace{x}_{[(M+1) \\times 1]} \\right) $$\nor in expanded form:\n$$ \\begin{bmatrix} a_{1} \\\\ a_{2} \\\\ \\vdots \\\\ a_{C} \\end{bmatrix} = \\sigma \\left( \\begin{bmatrix} W_{10} \u0026amp; W_{11} \u0026amp; W_{12} \u0026amp; \\dots \u0026amp; W_{1M} \\\\ W_{20} \u0026amp; W_{21} \u0026amp; W_{22} \u0026amp; \\dots \u0026amp; W_{2M} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ W_{C0} \u0026amp; W_{C1} \u0026amp; W_{C2} \u0026amp; \\dots \u0026amp; W_{CM} \\end{bmatrix} \\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{M} \\end{bmatrix} \\right) $$\nor in visual form:\nEach figure represents the same network, but highlights different output paths from the same inputs, and each path is computed by its own set of output weights.\nBatch Processing The computations described so far process only a single input example (batch size = 1). In practice, we process multiple inputs in parallel to improve computational efficiency.\nIf we process batch size of $N$ inputs simultaneously, then our input becomes $X \\in \\mathbb{R}^{N \\times (M+1)}$ (note the uppercase $X$ since we now have a matrix rather than a vector). To handle $N$ outputs while maintaining proper matrix dimensions, we transpose our weight matrix to $W \\in \\mathbb{R}^{(M+1) \\times C}$:\n$$ \\underbrace{A}_{[N \\times C]} = \\sigma\\left(\\underbrace{X}_{[N \\times (M+1)]} \\quad \\underbrace{W}_{[(M+1) \\times C]}\\right) $$\nor in matrix form:\n$$ \\footnotesize \\begin{bmatrix} A_{11} \u0026amp; A_{12} \u0026amp; \\dots \u0026amp; A_{1C} \\\\ A_{21} \u0026amp; A_{22} \u0026amp; \\dots \u0026amp; A_{2C} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ A_{N1} \u0026amp; A_{N2} \u0026amp; \\dots \u0026amp; A_{NC} \\\\ \\end{bmatrix} = \\sigma\\left( \\begin{bmatrix} X_{10} \u0026amp; X_{11} \u0026amp; \\dots \u0026amp; X_{1M} \\\\ X_{20} \u0026amp; X_{21} \u0026amp; \\dots \u0026amp; X_{2M} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ X_{N0} \u0026amp; X_{N1} \u0026amp; \\dots \u0026amp; X_{NM} \\\\ \\end{bmatrix} \\begin{bmatrix} W_{01} \u0026amp; W_{02} \u0026amp; \\dots \u0026amp; W_{0C} \\\\ W_{11} \u0026amp; W_{12} \u0026amp; \\dots \u0026amp; W_{1C} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ W_{M1} \u0026amp; W_{M2} \u0026amp; \\dots \u0026amp; W_{MC} \\end{bmatrix} \\right) $$\nWhen dealing with batches, it can sometimes be hard to picture how forward propagation works. We can visualize the process like this:\nNote that in this illustration, the non-linearity (activation function) is not explicitly shown. The final output $A$ actually corresponds to the activated values.\nMulti-layer Networks The calculations presented so far describe a single layer. Deep learning models stack multiple layers to learn increasingly complex representations. We can denote each layer with a superscript $[l]$ to represent the layer number:\n$$ \\begin{align*} \\underbrace{Z^{[l]}}_{[N \\times H_{l}]} \u0026amp;= \\underbrace{A^{[l-1]}}_{[N \\times H_{l-1}]} \\quad \\underbrace{W^{[l]}}_{[H_{l-1} \\times H_{l}]} \\\\ \\underbrace{A^{[l]}}_{[N \\times H_{l}]} \u0026amp;= \\underbrace{\\sigma(Z^{[l]})}_{[N \\times H_{l}]} \\end{align*} $$\nHere $H_l$ represents the number of hidden units in layer $l$. The input to layer $l$ activated output from the previous layer $l-1$. For simplicity, we usually define $A^{[0]} = X$ (the input layer).\nSoftmax for Classification For classification problems, we apply the softmax activation function to the final layer\u0026rsquo;s output to convert the raw scores into a probability distribution. The softmax function ensures that all outputs sum to 1, allowing us to interpret them as class probabilities.\nThe softmax function is applied row-wise (for each input example) across the $C$ classes:\n$$ \\hat{Y}_{ij} = \\text{softmax}(Z_{ij}) = \\frac{e^{Z_{ij}}}{\\sum_{l=1}^{C} e^{Z_{il}}} \\quad\\quad i = 1,2,\\dots,N \\quad \\text{ and } \\quad j =1,2,\\dots,C $$\nIntuitively, for input example $i$, this computes the probability of class $j$ by normalizing the exponential of its score by the sum of exponentials across all $C$ possible classes.\nThe matrix representation shows this row-wise operation:\n$$ \\footnotesize \\begin{bmatrix} \\hat{Y}_{11} \u0026amp; \\hat{Y}_{12} \u0026amp; \\dots \u0026amp; \\hat{Y}_{1C} \\\\ \\hat{Y}_{21} \u0026amp; \\hat{Y}_{22} \u0026amp; \\dots \u0026amp; \\hat{Y}_{2C} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\hat{Y}_{N1} \u0026amp; \\hat{Y}_{N2} \u0026amp; \\dots \u0026amp; \\hat{Y}_{NC} \\\\ \\end{bmatrix} = \\tiny \\begin{bmatrix} \\text{softmax}\\big( \u0026amp; Z_{11} \u0026amp; Z_{12} \u0026amp; \\dots \u0026amp; Z_{1C} \u0026amp; \\big) \\\\ \\text{softmax}\\big( \u0026amp; Z_{21} \u0026amp; Z_{22} \u0026amp; \\dots \u0026amp; Z_{2C} \u0026amp; \\big) \\\\ \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\text{softmax}\\big( \u0026amp; Z_{N1} \u0026amp; Z_{N2} \u0026amp; \\dots \u0026amp; Z_{NC} \u0026amp; \\big) \\\\ \\end{bmatrix} \\footnotesize = \\tiny \\begin{bmatrix} \\frac{e^{Z_{11}}}{\\sum_{l=1}^{C} e^{Z_{1l}}} \u0026amp; \\frac{e^{Z_{12}}}{\\sum_{l=1}^{C} e^{Z_{1l}}} \u0026amp; \\dots \u0026amp; \\frac{e^{Z_{1C}}}{\\sum_{l=1}^{C} e^{Z_{1l}}} \\\\ \u0026amp; \u0026amp; \u0026amp; \\\\ \\frac{e^{Z_{21}}}{\\sum_{l=1}^{C} e^{Z_{2l}}} \u0026amp; \\frac{e^{Z_{22}}}{\\sum_{l=1}^{C} e^{Z_{2l}}} \u0026amp; \\dots \u0026amp; \\frac{e^{Z_{2C}}}{\\sum_{l=1}^{C} e^{Z_{2l}}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \u0026amp; \u0026amp; \u0026amp; \\\\ \\frac{e^{Z_{N1}}}{\\sum_{l=1}^{C} e^{Z_{Nl}}} \u0026amp; \\frac{e^{Z_{N2}}}{\\sum_{l=1}^{C} e^{Z_{Nl}}} \u0026amp; \\dots \u0026amp; \\frac{e^{Z_{NC}}}{\\sum_{l=1}^{C} e^{Z_{Nl}}} \\\\ \\end{bmatrix} $$\nIf we denote the last layer as $L$, then the model’s output for $N$ inputs and $C$ classes can be represented as follows:\nCross-Entropy Loss Function After obtaining the predicted probabilities $\\hat{Y}$, we measure the model\u0026rsquo;s performance by comparing these predictions with the ground truth labels $Y \\in \\mathbb{R}^{N \\times C}$ using a loss function. The ground truth is represented as one-hot encoded vectors, where each input example has exactly one correct class. For multi-class classification, we typically use Cross-Entropy (CE) loss:\n$$ \\mathcal{L} = \\text{CE}(Y, \\hat{Y}) = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^C Y_{ij} \\log\\hat{Y}_{ij} $$\nThis formula computes the element-wise product between the true labels $Y$ and the logarithm of predicted probabilities $\\log\\hat{Y}$ then averages across all examples to produce a single scalar loss value.\nWe can express this using matrix operations with the element-wise (Hadamard) product $\\odot$:\n$$ \\underbrace{\\mathcal{L}}_{[1 \\times 1]} = \\text{CE}(Y, \\hat{Y}) = -\\frac{1}{N} \\quad \\underbrace{1^{T}}_{[1 \\times N]} \\quad (\\underbrace{Y \\odot \\log\\hat{Y}}_{[N \\times C]}) \\quad \\underbrace{1}_{[C \\times 1]} $$\nExpanding this matrix operation:\n$$ \\mathcal{L} = \\footnotesize -\\frac{1}{N} \\begin{bmatrix} 1_{1} \u0026amp; 1_{2} \u0026amp; \\dots \u0026amp; 1_{N} \\end{bmatrix} \\left(\\begin{bmatrix} Y_{11} \u0026amp; Y_{12} \u0026amp; \\dots \u0026amp; Y_{1C} \\\\ Y_{21} \u0026amp; Y_{22} \u0026amp; \\dots \u0026amp; Y_{2C} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ Y_{N1} \u0026amp; Y_{N2}\u0026amp; \\dots \u0026amp; Y_{NC} \\\\ \\end{bmatrix} \\odot \\begin{bmatrix} \\log\\hat{Y}_{11} \u0026amp; \\log\\hat{Y}_{12} \u0026amp; \\dots \u0026amp; \\log\\hat{Y}_{1C} \\\\ \\log\\hat{Y}_{21} \u0026amp; \\log\\hat{Y}_{22} \u0026amp; \\dots \u0026amp; \\log\\hat{Y}_{2C} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\log\\hat{Y}_{N1} \u0026amp; \\log\\hat{Y}_{N2} \u0026amp; \\dots \u0026amp; \\log\\hat{Y}_{NC} \\\\ \\end{bmatrix}\\right) \\begin{bmatrix} 1_{1} \\\\ 1_{2} \\\\ \\vdots \\\\ 1_{C} \\end{bmatrix} $$\nWe can simplify this by recognizing that multiplying by vectors of ones simply sums all elements in the matrix. Therefore:\n$$ \\mathcal{L} = \\small -\\frac{1}{N} \\ \\text{sum}\\left(\\begin{bmatrix} Y_{11}\\log\\hat{Y}_{11} \u0026amp; Y_{12}\\log\\hat{Y}_{12} \u0026amp; \\dots \u0026amp; Y_{1C}\\log\\hat{Y}_{1C} \\\\ Y_{21}\\log\\hat{Y}_{21} \u0026amp; Y_{22}\\log\\hat{Y}_{22} \u0026amp; \\dots \u0026amp; Y_{2C}\\log\\hat{Y}_{2C} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ Y_{N1}\\log\\hat{Y}_{N1} \u0026amp; Y_{N2}\\log\\hat{Y}_{N2} \u0026amp; \\dots \u0026amp; Y_{NC}\\log\\hat{Y}_{NC} \\\\ \\end{bmatrix}\\right) $$\nWe can show this summation with following image (excluding scaling factor $-\\frac{1}{N}$):\nAlthough this is a general representation, we denote the output cross-entropy loss as $-\\log\\hat{Y}$. Since the ground truth $Y$ is one-hot vector, it is 1 for true class and 0 for others in each input. Therefore, the assumption in this case can be:\n$$ \\mathcal{L}_{ij} = \\begin{cases} -\\log\\hat{Y}_{ij} \u0026amp; \\text{if } \\ Y_{ij} = 1 \\\\ 0 \u0026amp; \\text{otherwise} \\end{cases} \\quad\\quad\\text{for } \\ i=1,2,\\dots, N $$\nThis completes our mathematical framework for forward propagation in deep learning models, from single predictions to batch processing with multi-class classification and loss computation.\nSimple Computation Graph Before diving lots of matrix gradient calculation, I would like to show a computation graph for forward and backward propagation so we can see the cases we need to be careful when we compute gradients. In this case, I want to only show scalar calculations first instead of thinking about matrices so we can adapt this into matrix backpropagation.\n$$ \\begin{array}{rcl} a^{[0]} \u0026amp; = \u0026amp; x \\\\ \\\\ z^{[1]} \u0026amp; = \u0026amp; a^{[0]} \\cdot w^{[1]} \u0026amp; \\\\ a^{[1]} \u0026amp; = \u0026amp; \\sigma(z^{[1]}) \\\\ \\\\ z^{[2]} \u0026amp; = \u0026amp; a^{[1]} \\cdot w^{[2]} \u0026amp; \\\\ a^{[2]} \u0026amp; = \u0026amp; \\sigma(z^{[2]}) \\\\ \\\\ \\vdots \\\\ \\\\ z^{[l]} \u0026amp; = \u0026amp; a^{[l-1]} \\cdot w^{[l]} \u0026amp; \\\\ a^{[l]} \u0026amp; = \u0026amp; \\sigma(z^{[l]}) \\\\ \\\\ \\vdots \\\\ \\\\ z^{[L-1]} \u0026amp; = \u0026amp; a^{[L-2]} \\cdot w^{[L-1]} \u0026amp; \\\\ a^{[L-1]} \u0026amp; = \u0026amp; \\sigma(z^{[L-1]}) \\\\ \\\\ z^{[L]} \u0026amp; = \u0026amp; a^{[L-1]} \\cdot w^{[L]} \u0026amp; \\\\ \\hat{y} \u0026amp; = \u0026amp; \\text{softmax}(z^{[L]}) \\\\ \\mathcal{L} \u0026amp; = \u0026amp; \\text{CE}(\\hat{y}) \u0026amp; = -\\sum^{C}_{i=1} y_i \\log \\hat{y}_i \\end{array} $$\nwhere $[l]$ and $[L]$ represents arbitrary layer $l$ and last layer $L$, respectively. $\\sigma(\\cdot)$ is activation function (ReLU in this case). In this representation, everything is scalar except $w^{[L]}$ because the output must be multi-class for softmax so I just played in the last layer to show a meaningful example.\nWe can visualize the computation graph of this network as follows:\nChain Rule Backpropagation relies on the chain rule of calculus to compute gradients efficiently. For a composite function $f(g(h(x)))$, the chain rule states:\n$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial h} \\cdot \\frac{\\partial h}{\\partial x} $$\nIn neural networks, we apply this principle to decompose the loss gradient into a product of simpler derivatives based on any parameter.\nScalar Backward Propagation After computing loss, we can start computing gradients with respect to it. Since there are $C$ classes, we need to calculate derivative of each prediction $\\hat{y}_i$:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i} = \\frac{\\partial \\left( -y_i \\log\\hat{y}_i \\right)}{\\partial \\hat{y}_i} = -\\frac{y_i}{\\hat{y}_i} $$\nWe can visualize this gradient flow like this:\nSince the predicted output $\\hat{y}_i$ is computed using $z^{[L]}_i$ in both the numerator and the denominator\n$$ y_i = \\text{softmax}(z^{[L]}_i) = \\frac{e^{z^{[L]}_i}}{\\sum^{C}_{j=1} e^{z^{[L]}_j}}, $$\nthe derivative $\\frac{\\partial \\mathcal{L}}{\\partial z^{[L]}_k}$ depends not only on the numerator but also on the denominator:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial z^{[L]}_k} = \\sum_{i=1}^{C} \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} $$\nIt might be easier to understand when you see visual representation:\nSince we already know $\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i}$, the gradient expression simplifies to:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial z^{[L]}_k} = \\sum_{i=1}^{C} -\\frac{y_i}{\\hat{y}_i} \\cdot \\frac{\\partial \\hat{y_i}}{\\partial z^{[L]}_k} $$\nThe most confusing part begins here, because we now need to carefully compute the local gradient $\\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k}$. To do this, we apply the quotient rule:\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = \\frac{\\partial \\left(\\frac{e^{z^{[L]}_i}}{\\sum_{j=1}^C e^{z^{[L]}_j}} \\right)}{\\partial z^{[L]}_k} = \\frac{\\left(\\frac{\\partial e^{z^{[L]}_i}}{\\partial z^{[L]}_k}\\right) \\cdot \\sum_{j=1}^C e^{z^{[L]}_j} - e^{z^{[L]}_i} \\cdot \\left( \\frac{\\sum_{j=1}^C e^{z^{[L]}_j} }{\\partial z^{[L]}_k}\\right)} {\\left(\\sum_{j=1}^C e^{z^{[L]}_j} \\right)^2} $$\nFor this derivative, there are two distinct cases to consider:\n$$ i = k \\quad \\text{ or } \\quad i \\neq k $$\nCase 1: $i = k$ The derivative of numerator:\n$$ \\frac{\\partial e^{z^{[L]}_i}}{\\partial z^{[L]}_k} = e^{z^{[L]}_i} $$\nThe derivative of the denominator is:\n$$ \\frac{\\partial \\left(\\sum_{j=1}^C e^{z^{[L]}_j} \\right)}{\\partial z^{[L]}_k} = e^{z^{[L]}_k} $$\nSubstituting these into the quotient rule:\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = \\frac{z^{[L]}_i \\cdot \\sum_{j=1}^{C} z^{[L]}_j - z^{[L]}_i \\cdot z^{[L]}_k}{\\left( \\sum_{j=1}^{C} z^{[L]}_j \\right)^2} $$\nFactorizing into:\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = \\frac{e^{z^{[L]}_i}}{\\sum^{C}_{j=1} e^{z^{[L]}_j}} \\cdot \\left(1 - \\frac{e^{z^{[L]}_k}}{\\sum^{C}_{j=1} e^{z^{[L]}_j}} \\right) $$\nShortly, this is equivalent to: $$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = \\hat{y}_i \\cdot (1 - \\hat{y}_k) $$\nCase 2: $i \\mathrel{\\char`≠} k$ The derivative of the numerator is zero, because $e^{z_i}$ does not depend on $z_k$.\nThe derivative of the denominator is:\n$$ \\frac{\\partial \\left(\\sum_{j=1}^C e^{z^{[L]}_j} \\right)}{\\partial z^{[L]}_k} = e^{z^{[L]}_k} $$\nThus, we have:\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = \\frac{0 \\cdot \\sum_{j=1}^{C} z^{[L]}_j - z^{[L]}_i \\cdot z^{[L]}_k}{\\left( \\sum_{j=1}^{C} z^{[L]}_j \\right)^2} $$\nSimplifying:\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = -\\frac{e^{z^{[L]}_i}}{\\sum^{C}_{j=1} e^{z^{[L]}_j}} \\cdot \\frac{e^{z^{[L]}_k}}{\\sum^{C}_{j=1} e^{z^{[L]}_j}} $$\nShortly, this is equivalent to:\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = - \\hat{y}_i \\cdot \\hat{y}_k $$\nFinally, we can summarize these two cases as\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = \\begin{cases} \\hat{y}_i \\cdot (1 - \\hat{y}_k) \u0026amp; \\text{ if } i = k \\\\ -\\hat{y}_i \\cdot \\hat{y}_k \u0026amp; \\text{ if } i \\neq k \\end{cases} $$\nIn literature, it is pretty common to use the following form:\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = \\hat{y}_i \\cdot (\\delta_{ik} - \\hat{y}_k) $$\nwhere $\\delta_{ik}$ is 1 if $i = k$, 0 otherwise. When we combine the gradient of cross-entropy and local gradient, we have:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial z^{[L]}_k} = \\sum_{i=1}^{C} -\\frac{y_i}{\\hat{y}_i} \\cdot \\hat{y}_i \\cdot (\\delta_{ik} - \\hat{y}_k) $$\nwhere $\\hat{y}_i$ cancels each other:\n$$ \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial z^{[L]}_k} \u0026amp; = \u0026amp; \\sum_{i=1}^{C} -y_i \\cdot (\\delta_{ik} - \\hat{y}_k) \\\\ \u0026amp; = \u0026amp; -y_k + \\hat{y}_k \\sum_{i=1}^C y_i \\end{array} $$\nSince ground truth value $y_i$ is one-hot vector (1 for true class and 0 for others), equation simplifies to:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial z^{[L]}_k} = \\hat{y}_k - y_k $$\nThis compact form is what makes softmax combined with cross-entropy loss so useful. Instead of dealing with complicated fractions, we can directly use this result to propagate gradients to the next layers.\nFrom this point, gradient calculations across layers will start to follow a recurring pattern. Each layer essentially repeats the same process: we compute gradients with respect to its inputs and its weights.\nFor the layer output $z^{[L]}$, we have two key variables to differentiate with respect to:\nThe weights $w^{[L]}$: their gradients are crucial because they are the trainable parameters we want to optimize during learning.\nThe activations $a^{[L-1]}$: while not parameters themselves, their gradients are equally important since they ensure the flow of gradients backward through the network, enabling earlier layers to update as well.\nThus, even though only the weight gradients directly influence optimization, the activation gradients play a important role in keeping backpropagation alive throughout the entire network:\nIf we calculate derivatives with respect to weights $w^{[l]}$: $$ \\frac{\\partial \\mathcal{L}}{\\partial w^{[l]}} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} \\cdot \\frac{\\partial \\left( a^{[l-1]} \\cdot w^{[l]} \\right)}{\\partial w^{[l]}} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} \\cdot a^{[l-1]} $$\nIf we calculate derivatives with respect to weights $a^{[l-1]}$: $$ \\frac{\\partial \\mathcal{L}}{\\partial a^{[l-1]}} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} \\cdot \\frac{\\partial \\left( a^{[l-1]} \\cdot w^{[l]} \\right)}{\\partial a^{[l-1]}} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} \\cdot w^{[l]} $$\nLastly, we need to calculate the derivative of $a^{[l]}$ with respect to $z^{[l]}$ in $\\sigma(z^{[l]})$, assuming $\\sigma(z^{[l]}) = \\text{ReLU}(z^{[l]})$:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} = \\frac{\\partial \\mathcal{L}}{\\partial a^{[l]}} \\cdot \\frac{\\partial a^{[l]}}{\\partial z^{[l]}} = \\frac{\\partial \\mathcal{L}}{\\partial a^{[l]}} \\cdot 1(z^{[l]} \u0026gt; 0) $$\nwhere $1(z^{[l]} \u0026gt; 0)$ outputs 1 if $(z^{[l]} \u0026gt; 0)$, 0 otherwise as a indicator function.\nTherefore, we can summarize backpropagation for scalar values with following pattern:\n$$ \\begin{array}{rll} \\frac{\\partial \\mathcal{L}}{\\partial z^{[L]}} \u0026amp;= \\hat{y}-y \\\\ \\\\ \\vdots \\\\ \\\\ \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} \u0026amp;= \\frac{\\partial \\mathcal{L}}{\\partial a^{[l]}} \\cdot \\frac{\\partial a^{[l]}}{\\partial z^{[l]}} \u0026amp;= \\frac{\\partial \\mathcal{L}}{\\partial a^{[l]}} \\cdot 1(z^{[l]} \u0026gt; 0) \\\\ \\frac{\\partial \\mathcal{L}}{\\partial w^{[l]}} \u0026amp;= \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} \\cdot \\frac{\\partial z^{[l]}}{\\partial w^{[l]}} \u0026amp;= \\frac{\\partial \\mathcal{L}}{z^{[l]}} \\cdot a^{[l-1]} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial a^{[l-1]}} \u0026amp;= \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} \\cdot \\frac{\\partial z^{[l]}}{\\partial a^{[l-1]}} \u0026amp;= \\frac{\\partial \\mathcal{L}}{z^{[l]}} \\cdot w^{[l]} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial z^{[l-1]}} \u0026amp;= \\frac{\\partial \\mathcal{L}}{\\partial a^{[l-1]}} \\cdot \\frac{\\partial a^{[l-1]}}{\\partial z^{[l-1]}} \u0026amp;= \\frac{\\partial \\mathcal{L}}{\\partial a^{[l-1]}} \\cdot 1(z^{[l-1]} \u0026gt; 0) \\\\ \\\\ \\vdots \\\\ \\\\ \\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}} \u0026amp;= \\frac{\\partial \\mathcal{L}}{\\partial a^{[1]}} \\cdot \\frac{\\partial a^{[1]}}{\\partial z^{[1]}} \u0026amp;= \\frac{\\partial \\mathcal{L}}{\\partial a^{[1]}} \\cdot 1(z^{[1]} \u0026gt; 0) \\\\ \\frac{\\partial \\mathcal{L}}{\\partial w^{[1]}} \u0026amp;= \\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}} \\cdot \\frac{\\partial z^{[1]}}{\\partial w^{[1]}} \u0026amp;= \\frac{\\partial \\mathcal{L}}{z^{[1]}} \\cdot a^{[0]} \\\\ \\end{array} $$\nBackward Propagation After computing the loss through forward propagation, we need to update the model\u0026rsquo;s weights to minimize this loss. Backpropagation is the algorithm that computes the gradients of the loss function with respect to each parameter in the network. We\u0026rsquo;ll derive these gradients step by step, working backwards from the loss to the input layer.\nGradient of Cross-Entropy with Softmax Starting with loss function, we calculate relative gradients and go back step by step to calculate further gradients. Since the model has predictions with Softmax activation and the loss is calculated with cross-entropy, the combination of these methods has a nice property which simplifies the gradient:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}} = \\frac{1}{N} \\ \\underbrace{(\\hat{Y}-Y)}_{[N \\times C]} $$\nor in matrix representation:\n$$ \\footnotesize \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}} = \\frac{1}{N} \\ \\scriptsize \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{11}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{12}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{1C}} \\\\ \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{21}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{22}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{2C}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{N1}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{N2}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{NC}} \\\\ \\end{bmatrix} \\footnotesize = \\frac{1}{N} \\ \\begin{bmatrix} \\hat{Y}_{11} - Y_{11} \u0026amp; \\hat{Y}_{12} - Y_{12} \u0026amp; \\dots \u0026amp; \\hat{Y}_{1C} - Y_{1C} \\\\ \\hat{Y}_{21} - Y_{21} \u0026amp; \\hat{Y}_{22} - Y_{22} \u0026amp; \\dots \u0026amp; \\hat{Y}_{2C} - Y_{2C} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\hat{Y}_{N1} - Y_{N1} \u0026amp; \\hat{Y}_{N2} - Y_{N2} \u0026amp; \\dots \u0026amp; \\hat{Y}_{NC} - Y_{NC} \\\\ \\end{bmatrix} $$\nor in visual representation:\nGradient of Arbitrary Layer $l$ At this stage, we can begin calculating the gradients of the weights in the corresponding layer. Since the forward propagation is computed as\n$$ Z^{[l]} = A^{[l-1]} W^{[l]} $$\nThere are two possible local gradients we might consider:\n$$ \\frac{\\partial Z^{[l]}}{\\partial W^{[l]}} \\quad \\text{ or } \\quad \\frac{\\partial Z^{[l]}}{\\partial A^{[l-1]}} $$\nYou may wonder why we would need the gradient with respect to $A^{[l-1]}$ since our goal is to update the weight matrix $W^{[l]}$. The reason is that the derivative with respect to $A^{[l-1]}$ becomes necessary for updating the weights of the previous layer, because\n$$ A^{[l-1]} = \\sigma\\left(A^{[l-2]} W^{[l-1]} \\right) $$\nWhen we want to calculate the gradients of weight matrix $W^{[l]}$ with respect to the loss function $\\mathcal{L}$, we use chain rule as follows:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{\\text{Upstream Gradient }} \\ \\cdot \\ \\underbrace{\\frac{\\partial Z^{[l]}}{\\partial W^{[l]}}}_{ \\text{ Local Gradient}} $$\nThe local gradient gives us:\n$$ \\frac{\\partial Z^{[l]}}{\\partial W^{[l]}} = \\frac{\\partial \\left( A^{[l-1]} W^{[l]} \\right)}{\\partial W^{[l]}} = \\underbrace{A^{[l-1]}}_{[N \\times H_{l-1}]} $$\nSince there are $H_{l-1} \\times H_l$ values in weight matrix $W^{[l]}$, we need to find each individual gradient so the shape of $\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}$ should be $H_{l-1} \\times H_l$. However, the dimensions of matrices does not match when we try to calculate matrix multiplication:\n$$ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}}_{[H_{l-1} \\times H_{l}]} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} \\quad \\underbrace{A^{[l-1]}}_{[N \\times H_{l-1}]} $$\nIf we take transpose of $A^{[l-1]}$ and place it to the left side of upstream gradient, we have the correct dimension matching:\n$$ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}}_{[H_{l-1} \\times H_{l}]} = \\underbrace{{A^{[l-1]}}^T}_{[H_{l-1} \\times N]} \\ \\cdot \\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} $$\nAt first glance, this transformation may seem arbitrary. Why do we transpose? How can we be sure this result is correct? To clarify, let’s check the matrix representation:\n$$ \\scriptsize \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{11}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{12}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{1H_{l-1}}} \\\\ \\\\ \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{21}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{22}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{2H_{l-1}}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{H_{l}1}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{H_{l}2}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{H_{l}H_{l-1}}} \\\\ \\end{bmatrix} \\footnotesize = \\begin{bmatrix} {A^{[l-1]}_{11}}^T \u0026amp; {A^{[l-1]}_{12}}^T \u0026amp; \\dots \u0026amp; {A^{[l-1]}_{1N}}^T \\\\ {A^{[l-1]}_{21}}^T \u0026amp; {A^{[l-1]}_{22}}^T \u0026amp; \\dots \u0026amp; {A^{[l-1]}_{2N}}^T \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ {A^{[l-1]}_{H_{l-1}1}}^T \u0026amp; {A^{[l-1]}_{H_{l-1}2}}^T \u0026amp; \\dots \u0026amp; {A^{[l-1]}_{H_{l-1}N}}^T \\\\ \\end{bmatrix} \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{11}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{12}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{1H_{l}}} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{21}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{22}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{2H_{l}}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{N1}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{N2}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{NH_{l}}} \\\\ \\end{bmatrix} $$\nIf we look closely only one gradient calculation, such as $\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{11}}$:\n$$ \\footnotesize \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{11}} = \\begin{bmatrix} {A^{[l-1]}_{11}}^T \u0026amp; {A^{[l-1]}_{12}}^T \u0026amp; \\dots \u0026amp; {A^{[l-1]}_{1N}}^T \\\\ \\end{bmatrix} \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{11}} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{21}} \\\\ \\vdots \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{N1}} \\\\ \\end{bmatrix} $$\nSince single value of weight matrix is fixed for each input value, the gradient of this weight is just summation of all $N$ input derivatives:\n$$ \\footnotesize \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{11}} = {A^{[l-1]}_{11}}^T \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{11}} + {A^{[l-1]}_{12}}^T \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{21}} + \\dots + {A^{[l-1]}_{1N}}^T \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{N1}} $$\nIn the image below, you can see the visualization of this calculation for 4 weights in the network:\nAt the same time, we need to calculate the derivative of $A^{[l-1]}$, $\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}$, so upstream gradients can flow to previous layer and the gradient of weight matrix can be calculated in that layer:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial A^{[l-1]}} $$\nThe local gradient is:\n$$ \\frac{\\partial Z^{[L]}}{\\partial A^{[l-1]}} = \\frac{\\partial \\left(A^{[l-1]} W^{[l]} \\right)}{\\partial A^{[l-1]}} = \\underbrace{W^{[l]}}_{[H_{l-1} \\times H_{l}]} $$\nOnce again, the dimensions of matrices do not match:\n$$ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}}_{[N \\times H_{l-1}]} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} \\ \\cdot \\ \\underbrace{W^{[l]}}_{[H_{l-1} \\times H_{l}]} $$\nWe can transpose the weight matrix to align matrices:\n$$ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}}_{[N \\times H_{l-1}]} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} \\quad \\underbrace{W^{[l]^T}}_{[H_{l} \\times H_{l-1}]} $$\nWe can see the reasoning more easily if we check matrix representation again:\n$$ \\footnotesize \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{11}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{12}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{1H_{l-1}}} \\\\ \\\\ \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{21}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{22}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{2H_{l-1}}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{N1}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{N2}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{NH_{l-1}}} \\\\ \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{11}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{12}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{1H_{l}}} \\\\ \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{21}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{22}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{2H_{l}}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{N1}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{N2}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{NH_{l}}} \\\\ \\end{bmatrix} \\begin{bmatrix} {W^{[l]^T}_{11}} \u0026amp; {W^{[l]^T}_{12}} \u0026amp; \\dots \u0026amp; {W^{[l]^T}_{1H_{l-1}}} \\\\ {W^{[l]^T}_{21}} \u0026amp; {W^{[l]^T}_{22}} \u0026amp; \\dots \u0026amp; {W^{[l]^T}_{2H_{l-1}}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ {W^{[l]^T}_{H_{l}1}} \u0026amp; {W^{[l]^T}_{H_{l}2}} \u0026amp; \\dots \u0026amp; {W^{[l]^T}_{H_{l}H_{l-1}}} \\\\ \\end{bmatrix} $$\nBy focusing on only first value, $\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{11}}$, the gradient of $A^{l}_{11}$ depends on the values that it affected during forward propagation (the same logic as $\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{11}}$) :\n$$ \\footnotesize \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{11}} = \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{11}} \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{12}} \u0026amp; \\dots \u0026amp; \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{1H_{l}}} \\\\ \\end{bmatrix} \\begin{bmatrix} {W^{[l]^T}_{11}} \\\\ {W^{[l]^T}_{21}} \\\\ \\vdots \\\\ {W^{[l]^T}_{H_{l}1}} \\\\ \\end{bmatrix} $$\nNow we want to compute the gradient $\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l-1]}}$. Recall that $A^{[l-1]}$ is obtained by applying an activation function to $Z^{[l-1]}$:\n$$ \\underbrace{A^{[l-1]}}_{[N \\times H_{l-1}]} = \\underbrace{\\sigma(Z^{[l-1]})}_{[N \\times H_{l-1}]} $$\nIn our examples, we use the ReLU activation function. Its derivative with respect to each element is given by:\n$$ \\frac{\\partial \\ \\text{ReLU}(Z^{[l-1]}_{ij})}{\\partial Z^{[l-1]}_{ij}} = \\begin{cases} 1 \u0026amp; \\text{ if } \\ Z^{[l]}_{ij} \u0026gt; 0, \\\\ 0 \u0026amp; \\text{ otherwise } \\end{cases} $$\nFor $N \\times H_{l-1}$ inputs, we must compute a gradient for each element, producing a matrix of 0s and 1s. The derivative of ReLU applied element-wise can be written as the indicator (mask) of positive entries:\n$$ \\frac{\\partial \\ \\text{ReLU}(Z^{[l-1]})}{\\partial Z^{[l-1]}} = 1\\left(Z^{[l-1]} \u0026gt; 0\\right) $$\nwhere $1(\\cdot)$ is the element-wise indicator function (1 when the condition is true, 0 otherwise). In matrix form:\n$$ \\footnotesize \\frac{\\partial \\ \\text{ReLU}(Z^{[l-1]})}{\\partial Z^{[l-1]}} = \\begin{bmatrix} 1(Z^{[l-1]}_{11} \u0026gt; 0) \u0026amp; 1(Z^{[l-1]}_{12} \u0026gt; 0) \u0026amp; \\dots \u0026amp; 1(Z^{[l-1]}_{1H_{l-1}} \u0026gt; 0) \\\\ 1(Z^{[l-1]}_{21} \u0026gt; 0) \u0026amp; 1(Z^{[l-1]}_{22} \u0026gt; 0) \u0026amp; \\dots \u0026amp; 1(Z^{[l-1]}_{2H_{l-1}} \u0026gt; 0) \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1(Z^{[l-1]}_{N1} \u0026gt; 0) \u0026amp; 1(Z^{[l-1]}_{N2} \u0026gt; 0) \u0026amp; \\dots \u0026amp; 1(Z^{[l-1]}_{NH_{l-1}} \u0026gt; 0) \\\\ \\end{bmatrix} $$\nBecause the derivative is applied element-wise, the upstream gradient $\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}$ is combined with this mask element-wise, not by matrix multiplication:\n$$ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l-1]}}}_{[N \\times H_{l-1}]} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}}_{[N \\times H_{l-1}]} \\odot \\underbrace{\\frac{\\partial A^{[l-1]}}{\\partial Z^{[l-1]}}}_{[N \\times H_{l-1}]} $$\nor\n$$ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l-1]}}}_{[N \\times H_{l-1}]} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}}_{[N \\times H_{l-1}]} \\odot \\underbrace{1\\left(Z^{[l-1]} \u0026gt; 0\\right)}_{[N \\times H_{l-1}]} $$\nThe Pattern At this point, we completed the backpropagation using the matrix multiplication step. From here on, the process is essentially a repetition: each layer applies the same logic and propagates the gradients backwards through the weights (also biases implicitly) and activations.\nThe only real exception was the output layer, where we combined the softmax function with cross-entropy loss. This required a more detailed derivation, but the model is consistent for all hidden layers and is repeated across the entire network. You can see the pattern as follows:\n$$ \\footnotesize \\begin{array}{rll} \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}}}_{[N \\times C]} \u0026amp;= \\frac{1}{N} \\underbrace{\\left(\\hat{Y} - Y\\right)}_{N \\times C} \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial W^{[L]}}}_{[H_{L-1} \\times C]} \u0026amp;= \\underbrace{\\frac{\\partial \\left(A^{[L-1]} W^{[L]} \\right)}{\\partial W^{[L]}}}_{[H_{L-1} \\times N]} \\quad \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}}}_{[N \\times C]} \u0026amp;= \\underbrace{A^{[L-1]^T}}_{[H_{L-1} \\times N]} \\quad \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}}}_{[N \\times C]} \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[L-1]}}}_{[N \\times H_{L-1}]} \u0026amp;= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}}}_{[N \\times C]} \\quad \\underbrace{\\frac{\\partial \\left(A^{[L-1]} W^{[L]} \\right)}{\\partial A^{[L-1]}}}_{[C \\times H_{L-1}]} \u0026amp;= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}}}_{[N \\times C]} \\quad \\underbrace{W^{[L]^T}}_{[C \\times H_{L-1}]} \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[L-1]}}}_{[N \\times H_{L-1}]} \u0026amp;= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[L-1]}}}_{[N \\times H_{L-1}]} \\odot \\underbrace{\\frac{\\partial \\left(\\sigma\\left(Z^{[L-1]}\\right) \\right)}{\\partial Z^{[L-1]}}}_{[N \\times H_{L-1}]} \u0026amp;= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[L-1]}}}_{[N \\times H_{L-1}]} \\odot \\underbrace{1 \\left( Z^{[L-1]} \u0026gt; 0 \\right)}_{[N \\times H_{L-1}]} \\\\ \\\\ \\vdots \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}}_{[H_{l-1} \\times H_{l}]} \u0026amp;=\\underbrace{\\frac{\\partial \\left(A^{[l-1]} W^{[l]} \\right)}{\\partial W^{[l]}}}_{[H_{l-1} \\times N]} \\quad \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} \u0026amp;= \\underbrace{A^{[l-1]^T}}_{[H_{l-1} \\times N]} \\quad \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}}_{[N \\times H_{l-1}]} \u0026amp;= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} \\quad \\underbrace{\\frac{\\partial \\left(A^{[l-1]} W^{[l]} \\right)}{\\partial A^{[l-1]}}}_{[H_{l} \\times H_{l-1}]} \u0026amp;= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} \\quad \\underbrace{W^{[l]^T}}_{[H_{l} \\times H_{l-1}]} \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l-1]}}}_{[N \\times H_{l-1}]} \u0026amp;= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}}_{[N \\times H_{l-1}]} \\odot \\underbrace{\\frac{\\partial \\left(\\sigma\\left(Z^{[l-1]}\\right) \\right)}{\\partial Z^{[l-1]}}}_{[N \\times H_{l-1}]} \u0026amp;= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}}_{[N \\times H_{l-1}]} \\odot \\underbrace{1 \\left( Z^{[l-1]} \u0026gt; 0 \\right)}_{[N \\times H_{l-1}]} \\\\ \\\\ \\vdots \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[1]}}}_{[N \\times H_{1}]} \u0026amp;= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[1]}}}_{[N \\times H_{1}]} \\odot \\underbrace{\\frac{\\partial \\left(\\sigma\\left(Z^{[1]}\\right) \\right)}{\\partial Z^{[1]}}}_{[N \\times H_{1}]} \u0026amp;= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[1]}}}_{[N \\times H_{1}]} \\odot \\underbrace{1 \\left( Z^{[1]} \u0026gt; 0 \\right)}_{[N \\times H_{1}]} \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}}}_{[H_{0} \\times H_{1}]} \u0026amp;= \\underbrace{\\frac{\\partial \\left(A^{[0]} W^{[1]} \\right)}{\\partial W^{[1]}}}_{[H_{0} \\times N]} \\quad \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[1]}}}_{[N \\times H_{1}]} \u0026amp;= \\underbrace{A^{[0]^T}}_{[H_{0} \\times N]} \\quad \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[1]}}}_{[N \\times H_{1}]} \\\\ \\end{array} $$\nI know it looks ugly but if you pay some attention, you will see the pattern for gradient calculation. There are just some rules that you need to follow for each block.\n","permalink":"https://umutkavakli.github.io/posts/2025-09-18-backpropagation-through-matrix-multiplication/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e \u003cbr\u003e\nIn this long and technical blog, I explained:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eForward propagation:\u003c/strong\u003e How inputs flow through the network layer by layer (using matrix operations) to generate predictions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eComputation Graph:\u003c/strong\u003e How simple scalar examples help visualize backpropagation and build intuition before scaling up.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eBackward propagation:\u003c/strong\u003e How errors flow backward (again with matrix operations) to compute gradients and update weights.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003eBackpropagation appears quite straightforward when working with scalars or even simple vectors. However, once we step into the world of matrices, things quickly become more complex and difficult to follow. There are extra details and notations that make it less intuitive.\u003c/p\u003e","title":"Backpropagation Through Matrix Multiplication"},{"content":"Coming soon!\n","permalink":"https://umutkavakli.github.io/projects/","summary":"\u003cp\u003eComing soon!\u003c/p\u003e","title":"Projects"}]