<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Backpropagation Through Matrix Multiplication | umut&#39;s blog</title>
<meta name="keywords" content="">
<meta name="description" content="TL;DR 
In this long and technical blog, I explained:


Forward propagation: How inputs flow through the network layer by layer (using matrix operations) to generate predictions.


Computation Graph: How simple scalar examples help visualize backpropagation and build intuition before scaling up.


Backward propagation: How errors flow backward (again with matrix operations) to compute gradients and update weights.



Backpropagation appears quite straightforward when working with scalars or even simple vectors. However, once we step into the world of matrices, things quickly become more complex and difficult to follow. There are extra details and notations that make it less intuitive.">
<meta name="author" content="Author: Umut Kaan Kavaklı">
<link rel="canonical" href="https://umutkavakli.github.io/posts/2025-09-18-backpropagation-through-matrix-multiplication/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://umutkavakli.github.io/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://umutkavakli.github.io/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://umutkavakli.github.io/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://umutkavakli.github.io/assets/apple-touch-icon.png">
<link rel="mask-icon" href="https://umutkavakli.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://umutkavakli.github.io/posts/2025-09-18-backpropagation-through-matrix-multiplication/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>
<meta property="og:url" content="https://umutkavakli.github.io/posts/2025-09-18-backpropagation-through-matrix-multiplication/">
  <meta property="og:site_name" content="umut&#39;s blog">
  <meta property="og:title" content="Backpropagation Through Matrix Multiplication">
  <meta property="og:description" content="TL;DR In this long and technical blog, I explained:
Forward propagation: How inputs flow through the network layer by layer (using matrix operations) to generate predictions.
Computation Graph: How simple scalar examples help visualize backpropagation and build intuition before scaling up.
Backward propagation: How errors flow backward (again with matrix operations) to compute gradients and update weights.
Backpropagation appears quite straightforward when working with scalars or even simple vectors. However, once we step into the world of matrices, things quickly become more complex and difficult to follow. There are extra details and notations that make it less intuitive.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-09-18T12:05:30+03:00">
    <meta property="article:modified_time" content="2025-09-18T12:05:30+03:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Backpropagation Through Matrix Multiplication">
<meta name="twitter:description" content="TL;DR 
In this long and technical blog, I explained:


Forward propagation: How inputs flow through the network layer by layer (using matrix operations) to generate predictions.


Computation Graph: How simple scalar examples help visualize backpropagation and build intuition before scaling up.


Backward propagation: How errors flow backward (again with matrix operations) to compute gradients and update weights.



Backpropagation appears quite straightforward when working with scalars or even simple vectors. However, once we step into the world of matrices, things quickly become more complex and difficult to follow. There are extra details and notations that make it less intuitive.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://umutkavakli.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Backpropagation Through Matrix Multiplication",
      "item": "https://umutkavakli.github.io/posts/2025-09-18-backpropagation-through-matrix-multiplication/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Backpropagation Through Matrix Multiplication",
  "name": "Backpropagation Through Matrix Multiplication",
  "description": "TL;DR In this long and technical blog, I explained:\nForward propagation: How inputs flow through the network layer by layer (using matrix operations) to generate predictions.\nComputation Graph: How simple scalar examples help visualize backpropagation and build intuition before scaling up.\nBackward propagation: How errors flow backward (again with matrix operations) to compute gradients and update weights.\nBackpropagation appears quite straightforward when working with scalars or even simple vectors. However, once we step into the world of matrices, things quickly become more complex and difficult to follow. There are extra details and notations that make it less intuitive.\n",
  "keywords": [
    
  ],
  "articleBody": "TL;DR In this long and technical blog, I explained:\nForward propagation: How inputs flow through the network layer by layer (using matrix operations) to generate predictions.\nComputation Graph: How simple scalar examples help visualize backpropagation and build intuition before scaling up.\nBackward propagation: How errors flow backward (again with matrix operations) to compute gradients and update weights.\nBackpropagation appears quite straightforward when working with scalars or even simple vectors. However, once we step into the world of matrices, things quickly become more complex and difficult to follow. There are extra details and notations that make it less intuitive.\nPersonally, although I managed to understand this concept while preparing for deep learning exams, I usually lose my intuition a few months later. Reviewing it again requires extra effort to bring everything back together. That’s why I decided to write this technical blog post, both as a reminder for myself and as a guide for anyone trying to understand backpropagation in matrix form.\nForward Propagation When training a deep learning model, the first step is to compute a linear combination of input features. Given an input vector $x \\in \\mathbb{R}^M$ with $M$ features and a corresponding weight vector $w \\in \\mathbb{R}^M$, we calculate a weighted sum and add a bias term $b \\in \\mathbb{R}^1$ to produce a single output $z$:\n$$ z = w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\dots + w_M \\cdot x_M + b $$\nThis is simply the dot product between $w$ and $x$ with an added bias term:\n$$ z = w^T x + b $$\nTo visualize this computation, we can represent it in matrix form:\n$$ \\begin{bmatrix} z \\end{bmatrix} = \\begin{bmatrix} w_1 \u0026 w_2 \u0026 \\cdots \u0026 w_M \\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_M \\end{bmatrix} + \\begin{bmatrix} b \\end{bmatrix} $$\nor in image representation:\nWe can simplify our notation by incorporating the bias term directly into the weight vector. This involves adding the bias $b$ as the first element of the weight vector and including a constant input $x_0 = 1$:\n$$ \\begin{bmatrix} z \\end{bmatrix} = \\begin{bmatrix} b \u0026 w_1 \u0026 w_2 \u0026 \\cdots \u0026 w_M \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_M \\end{bmatrix} $$\nFor notational clarity, we’ll denote the bias term as $w_0 = b$ and the constant input as $x_0 = 1$. Under this convention, both vectors have dimension $M+1$: $w \\in \\mathbb{R}^{M+1}$ and $x \\in \\mathbb{R}^{M+1}$. Our equation becomes:\n$$ z = w^T x $$\nor in matrix form:\n$$ \\begin{bmatrix} z \\end{bmatrix} = \\begin{bmatrix} w_0 \u0026 w_1 \u0026 w_2 \u0026 \\cdots \u0026 w_M \\end{bmatrix} \\cdot \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_M \\end{bmatrix} $$\nor in image representation:\nAdding Non-Linearity The linear combination alone is insufficient for learning complex patterns. To add non-linearity, we apply an activation function $\\sigma(\\cdot)$ to the output $z$.\n$$ a = \\sigma(z) = \\sigma(w^Tx) $$\nAlthough $\\sigma$ often denotes the Sigmoid function, here it represents a general activation function. For this explanation, we’ll use the ReLU activation function in hidden layers due to its simplicity, computational efficiency, resistance to vanishing gradients, and widespread popularity:\n$$ a = \\sigma(z) = \\max(0, z) $$\nwe can show this with visual representation:\nMulti-class Classification In practice, deep learning models often solve multi-class problems where we need to predict one of $C$ possible classes. This requires the output $a$ to be $C$-dimensional vector, with each dimension representing the score for a particular class. The predicted class corresponds to the entry with the highest activation value.\nTo generate $C$ outputs, we need $C$ distinct weight vectors, each of dimension $(M+1)$. To achieve this, we need a separate weight vector for each class. Stacking them gives a weight matrix $W \\in \\mathbb{R}^{C \\times (M+1)}$:\n$$ \\underbrace{a}_{[C \\times 1]} = \\sigma \\left( \\underbrace{W}_{[C \\times (M+1)]} \\quad \\underbrace{x}_{[(M+1) \\times 1]} \\right) $$\nor in expanded form:\n$$ \\begin{bmatrix} a_{1} \\\\ a_{2} \\\\ \\vdots \\\\ a_{C} \\end{bmatrix} = \\sigma \\left( \\begin{bmatrix} W_{10} \u0026 W_{11} \u0026 W_{12} \u0026 \\dots \u0026 W_{1M} \\\\ W_{20} \u0026 W_{21} \u0026 W_{22} \u0026 \\dots \u0026 W_{2M} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ W_{C0} \u0026 W_{C1} \u0026 W_{C2} \u0026 \\dots \u0026 W_{CM} \\end{bmatrix} \\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{M} \\end{bmatrix} \\right) $$\nor in visual form:\nEach figure represents the same network, but highlights different output paths from the same inputs, and each path is computed by its own set of output weights.\nBatch Processing The computations described so far process only a single input example (batch size = 1). In practice, we process multiple inputs in parallel to improve computational efficiency.\nIf we process batch size of $N$ inputs simultaneously, then our input becomes $X \\in \\mathbb{R}^{N \\times (M+1)}$ (note the uppercase $X$ since we now have a matrix rather than a vector). To handle $N$ outputs while maintaining proper matrix dimensions, we transpose our weight matrix to $W \\in \\mathbb{R}^{(M+1) \\times C}$:\n$$ \\underbrace{A}_{[N \\times C]} = \\sigma\\left(\\underbrace{X}_{[N \\times (M+1)]} \\quad \\underbrace{W}_{[(M+1) \\times C]}\\right) $$\nor in matrix form:\n$$ \\footnotesize \\begin{bmatrix} A_{11} \u0026 A_{12} \u0026 \\dots \u0026 A_{1C} \\\\ A_{21} \u0026 A_{22} \u0026 \\dots \u0026 A_{2C} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ A_{N1} \u0026 A_{N2} \u0026 \\dots \u0026 A_{NC} \\\\ \\end{bmatrix} = \\sigma\\left( \\begin{bmatrix} X_{10} \u0026 X_{11} \u0026 \\dots \u0026 X_{1M} \\\\ X_{20} \u0026 X_{21} \u0026 \\dots \u0026 X_{2M} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ X_{N0} \u0026 X_{N1} \u0026 \\dots \u0026 X_{NM} \\\\ \\end{bmatrix} \\begin{bmatrix} W_{01} \u0026 W_{02} \u0026 \\dots \u0026 W_{0C} \\\\ W_{11} \u0026 W_{12} \u0026 \\dots \u0026 W_{1C} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ W_{M1} \u0026 W_{M2} \u0026 \\dots \u0026 W_{MC} \\end{bmatrix} \\right) $$\nWhen dealing with batches, it can sometimes be hard to picture how forward propagation works. We can visualize the process like this:\nNote that in this illustration, the non-linearity (activation function) is not explicitly shown. The final output $A$ actually corresponds to the activated values.\nMulti-layer Networks The calculations presented so far describe a single layer. Deep learning models stack multiple layers to learn increasingly complex representations. We can denote each layer with a superscript $[l]$ to represent the layer number:\n$$ \\begin{align*} \\underbrace{Z^{[l]}}_{[N \\times H_{l}]} \u0026= \\underbrace{A^{[l-1]}}_{[N \\times H_{l-1}]} \\quad \\underbrace{W^{[l]}}_{[H_{l-1} \\times H_{l}]} \\\\ \\underbrace{A^{[l]}}_{[N \\times H_{l}]} \u0026= \\underbrace{\\sigma(Z^{[l]})}_{[N \\times H_{l}]} \\end{align*} $$\nHere $H_l$ represents the number of hidden units in layer $l$. The input to layer $l$ activated output from the previous layer $l-1$. For simplicity, we usually define $A^{[0]} = X$ (the input layer).\nSoftmax for Classification For classification problems, we apply the softmax activation function to the final layer’s output to convert the raw scores into a probability distribution. The softmax function ensures that all outputs sum to 1, allowing us to interpret them as class probabilities.\nThe softmax function is applied row-wise (for each input example) across the $C$ classes:\n$$ \\hat{Y}_{ij} = \\text{softmax}(Z_{ij}) = \\frac{e^{Z_{ij}}}{\\sum_{l=1}^{C} e^{Z_{il}}} \\quad\\quad i = 1,2,\\dots,N \\quad \\text{ and } \\quad j =1,2,\\dots,C $$\nIntuitively, for input example $i$, this computes the probability of class $j$ by normalizing the exponential of its score by the sum of exponentials across all $C$ possible classes.\nThe matrix representation shows this row-wise operation:\n$$ \\footnotesize \\begin{bmatrix} \\hat{Y}_{11} \u0026 \\hat{Y}_{12} \u0026 \\dots \u0026 \\hat{Y}_{1C} \\\\ \\hat{Y}_{21} \u0026 \\hat{Y}_{22} \u0026 \\dots \u0026 \\hat{Y}_{2C} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\hat{Y}_{N1} \u0026 \\hat{Y}_{N2} \u0026 \\dots \u0026 \\hat{Y}_{NC} \\\\ \\end{bmatrix} = \\tiny \\begin{bmatrix} \\text{softmax}\\big( \u0026 Z_{11} \u0026 Z_{12} \u0026 \\dots \u0026 Z_{1C} \u0026 \\big) \\\\ \\text{softmax}\\big( \u0026 Z_{21} \u0026 Z_{22} \u0026 \\dots \u0026 Z_{2C} \u0026 \\big) \\\\ \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\text{softmax}\\big( \u0026 Z_{N1} \u0026 Z_{N2} \u0026 \\dots \u0026 Z_{NC} \u0026 \\big) \\\\ \\end{bmatrix} \\footnotesize = \\tiny \\begin{bmatrix} \\frac{e^{Z_{11}}}{\\sum_{l=1}^{C} e^{Z_{1l}}} \u0026 \\frac{e^{Z_{12}}}{\\sum_{l=1}^{C} e^{Z_{1l}}} \u0026 \\dots \u0026 \\frac{e^{Z_{1C}}}{\\sum_{l=1}^{C} e^{Z_{1l}}} \\\\ \u0026 \u0026 \u0026 \\\\ \\frac{e^{Z_{21}}}{\\sum_{l=1}^{C} e^{Z_{2l}}} \u0026 \\frac{e^{Z_{22}}}{\\sum_{l=1}^{C} e^{Z_{2l}}} \u0026 \\dots \u0026 \\frac{e^{Z_{2C}}}{\\sum_{l=1}^{C} e^{Z_{2l}}} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \u0026 \u0026 \u0026 \\\\ \\frac{e^{Z_{N1}}}{\\sum_{l=1}^{C} e^{Z_{Nl}}} \u0026 \\frac{e^{Z_{N2}}}{\\sum_{l=1}^{C} e^{Z_{Nl}}} \u0026 \\dots \u0026 \\frac{e^{Z_{NC}}}{\\sum_{l=1}^{C} e^{Z_{Nl}}} \\\\ \\end{bmatrix} $$\nIf we denote the last layer as $L$, then the model’s output for $N$ inputs and $C$ classes can be represented as follows:\nCross-Entropy Loss Function After obtaining the predicted probabilities $\\hat{Y}$, we measure the model’s performance by comparing these predictions with the ground truth labels $Y \\in \\mathbb{R}^{N \\times C}$ using a loss function. The ground truth is represented as one-hot encoded vectors, where each input example has exactly one correct class. For multi-class classification, we typically use Cross-Entropy (CE) loss:\n$$ \\mathcal{L} = \\text{CE}(Y, \\hat{Y}) = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^C Y_{ij} \\log\\hat{Y}_{ij} $$\nThis formula computes the element-wise product between the true labels $Y$ and the logarithm of predicted probabilities $\\log\\hat{Y}$ then averages across all examples to produce a single scalar loss value.\nWe can express this using matrix operations with the element-wise (Hadamard) product $\\odot$:\n$$ \\underbrace{\\mathcal{L}}_{[1 \\times 1]} = \\text{CE}(Y, \\hat{Y}) = -\\frac{1}{N} \\quad \\underbrace{1^{T}}_{[1 \\times N]} \\quad (\\underbrace{Y \\odot \\log\\hat{Y}}_{[N \\times C]}) \\quad \\underbrace{1}_{[C \\times 1]} $$\nExpanding this matrix operation:\n$$ \\mathcal{L} = \\footnotesize -\\frac{1}{N} \\begin{bmatrix} 1_{1} \u0026 1_{2} \u0026 \\dots \u0026 1_{N} \\end{bmatrix} \\left(\\begin{bmatrix} Y_{11} \u0026 Y_{12} \u0026 \\dots \u0026 Y_{1C} \\\\ Y_{21} \u0026 Y_{22} \u0026 \\dots \u0026 Y_{2C} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ Y_{N1} \u0026 Y_{N2}\u0026 \\dots \u0026 Y_{NC} \\\\ \\end{bmatrix} \\odot \\begin{bmatrix} \\log\\hat{Y}_{11} \u0026 \\log\\hat{Y}_{12} \u0026 \\dots \u0026 \\log\\hat{Y}_{1C} \\\\ \\log\\hat{Y}_{21} \u0026 \\log\\hat{Y}_{22} \u0026 \\dots \u0026 \\log\\hat{Y}_{2C} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\log\\hat{Y}_{N1} \u0026 \\log\\hat{Y}_{N2} \u0026 \\dots \u0026 \\log\\hat{Y}_{NC} \\\\ \\end{bmatrix}\\right) \\begin{bmatrix} 1_{1} \\\\ 1_{2} \\\\ \\vdots \\\\ 1_{C} \\end{bmatrix} $$\nWe can simplify this by recognizing that multiplying by vectors of ones simply sums all elements in the matrix. Therefore:\n$$ \\mathcal{L} = \\small -\\frac{1}{N} \\ \\text{sum}\\left(\\begin{bmatrix} Y_{11}\\log\\hat{Y}_{11} \u0026 Y_{12}\\log\\hat{Y}_{12} \u0026 \\dots \u0026 Y_{1C}\\log\\hat{Y}_{1C} \\\\ Y_{21}\\log\\hat{Y}_{21} \u0026 Y_{22}\\log\\hat{Y}_{22} \u0026 \\dots \u0026 Y_{2C}\\log\\hat{Y}_{2C} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ Y_{N1}\\log\\hat{Y}_{N1} \u0026 Y_{N2}\\log\\hat{Y}_{N2} \u0026 \\dots \u0026 Y_{NC}\\log\\hat{Y}_{NC} \\\\ \\end{bmatrix}\\right) $$\nWe can show this summation with following image (excluding scaling factor $-\\frac{1}{N}$):\nAlthough this is a general representation, we denote the output cross-entropy loss as $-\\log\\hat{Y}$. Since the ground truth $Y$ is one-hot vector, it is 1 for true class and 0 for others in each input. Therefore, the assumption in this case can be:\n$$ \\mathcal{L}_{ij} = \\begin{cases} -\\log\\hat{Y}_{ij} \u0026 \\text{if } \\ Y_{ij} = 1 \\\\ 0 \u0026 \\text{otherwise} \\end{cases} \\quad\\quad\\text{for } \\ i=1,2,\\dots, N $$\nThis completes our mathematical framework for forward propagation in deep learning models, from single predictions to batch processing with multi-class classification and loss computation.\nSimple Computation Graph Before diving lots of matrix gradient calculation, I would like to show a computation graph for forward and backward propagation so we can see the cases we need to be careful when we compute gradients. In this case, I want to only show scalar calculations first instead of thinking about matrices so we can adapt this into matrix backpropagation.\n$$ \\begin{array}{rcl} a^{[0]} \u0026 = \u0026 x \\\\ \\\\ z^{[1]} \u0026 = \u0026 a^{[0]} \\cdot w^{[1]} \u0026 \\\\ a^{[1]} \u0026 = \u0026 \\sigma(z^{[1]}) \\\\ \\\\ z^{[2]} \u0026 = \u0026 a^{[1]} \\cdot w^{[2]} \u0026 \\\\ a^{[2]} \u0026 = \u0026 \\sigma(z^{[2]}) \\\\ \\\\ \\vdots \\\\ \\\\ z^{[l]} \u0026 = \u0026 a^{[l-1]} \\cdot w^{[l]} \u0026 \\\\ a^{[l]} \u0026 = \u0026 \\sigma(z^{[l]}) \\\\ \\\\ \\vdots \\\\ \\\\ z^{[L-1]} \u0026 = \u0026 a^{[L-2]} \\cdot w^{[L-1]} \u0026 \\\\ a^{[L-1]} \u0026 = \u0026 \\sigma(z^{[L-1]}) \\\\ \\\\ z^{[L]} \u0026 = \u0026 a^{[L-1]} \\cdot w^{[L]} \u0026 \\\\ \\hat{y} \u0026 = \u0026 \\text{softmax}(z^{[L]}) \\\\ \\mathcal{L} \u0026 = \u0026 \\text{CE}(\\hat{y}) \u0026 = -\\sum^{C}_{i=1} y_i \\log \\hat{y}_i \\end{array} $$\nwhere $[l]$ and $[L]$ represents arbitrary layer $l$ and last layer $L$, respectively. $\\sigma(\\cdot)$ is activation function (ReLU in this case). In this representation, everything is scalar except $w^{[L]}$ because the output must be multi-class for softmax so I just played in the last layer to show a meaningful example.\nWe can visualize the computation graph of this network as follows:\nChain Rule Backpropagation relies on the chain rule of calculus to compute gradients efficiently. For a composite function $f(g(h(x)))$, the chain rule states:\n$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial h} \\cdot \\frac{\\partial h}{\\partial x} $$\nIn neural networks, we apply this principle to decompose the loss gradient into a product of simpler derivatives based on any parameter.\nScalar Backward Propagation After computing loss, we can start computing gradients with respect to it. Since there are $C$ classes, we need to calculate derivative of each prediction $\\hat{y}_i$:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i} = \\frac{\\partial \\left( -y_i \\log\\hat{y}_i \\right)}{\\partial \\hat{y}_i} = -\\frac{y_i}{\\hat{y}_i} $$\nWe can visualize this gradient flow like this:\nSince the predicted output $\\hat{y}_i$ is computed using $z^{[L]}_i$ in both the numerator and the denominator\n$$ y_i = \\text{softmax}(z^{[L]}_i) = \\frac{e^{z^{[L]}_i}}{\\sum^{C}_{j=1} e^{z^{[L]}_j}}, $$\nthe derivative $\\frac{\\partial \\mathcal{L}}{\\partial z^{[L]}_k}$ depends not only on the numerator but also on the denominator:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial z^{[L]}_k} = \\sum_{i=1}^{C} \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} $$\nIt might be easier to understand when you see visual representation:\nSince we already know $\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i}$, the gradient expression simplifies to:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial z^{[L]}_k} = \\sum_{i=1}^{C} -\\frac{y_i}{\\hat{y}_i} \\cdot \\frac{\\partial \\hat{y_i}}{\\partial z^{[L]}_k} $$\nThe most confusing part begins here, because we now need to carefully compute the local gradient $\\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k}$. To do this, we apply the quotient rule:\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = \\frac{\\partial \\left(\\frac{e^{z^{[L]}_i}}{\\sum_{j=1}^C e^{z^{[L]}_j}} \\right)}{\\partial z^{[L]}_k} = \\frac{\\left(\\frac{\\partial e^{z^{[L]}_i}}{\\partial z^{[L]}_k}\\right) \\cdot \\sum_{j=1}^C e^{z^{[L]}_j} - e^{z^{[L]}_i} \\cdot \\left( \\frac{\\sum_{j=1}^C e^{z^{[L]}_j} }{\\partial z^{[L]}_k}\\right)} {\\left(\\sum_{j=1}^C e^{z^{[L]}_j} \\right)^2} $$\nFor this derivative, there are two distinct cases to consider:\n$$ i = k \\quad \\text{ or } \\quad i \\neq k $$\nCase 1: $i = k$ The derivative of numerator:\n$$ \\frac{\\partial e^{z^{[L]}_i}}{\\partial z^{[L]}_k} = e^{z^{[L]}_i} $$\nThe derivative of the denominator is:\n$$ \\frac{\\partial \\left(\\sum_{j=1}^C e^{z^{[L]}_j} \\right)}{\\partial z^{[L]}_k} = e^{z^{[L]}_k} $$\nSubstituting these into the quotient rule:\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = \\frac{z^{[L]}_i \\cdot \\sum_{j=1}^{C} z^{[L]}_j - z^{[L]}_i \\cdot z^{[L]}_k}{\\left( \\sum_{j=1}^{C} z^{[L]}_j \\right)^2} $$\nFactorizing into:\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = \\frac{e^{z^{[L]}_i}}{\\sum^{C}_{j=1} e^{z^{[L]}_j}} \\cdot \\left(1 - \\frac{e^{z^{[L]}_k}}{\\sum^{C}_{j=1} e^{z^{[L]}_j}} \\right) $$\nShortly, this is equivalent to: $$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = \\hat{y}_i \\cdot (1 - \\hat{y}_k) $$\nCase 2: $i \\mathrel{\\char`≠} k$ The derivative of the numerator is zero, because $e^{z_i}$ does not depend on $z_k$.\nThe derivative of the denominator is:\n$$ \\frac{\\partial \\left(\\sum_{j=1}^C e^{z^{[L]}_j} \\right)}{\\partial z^{[L]}_k} = e^{z^{[L]}_k} $$\nThus, we have:\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = \\frac{0 \\cdot \\sum_{j=1}^{C} z^{[L]}_j - z^{[L]}_i \\cdot z^{[L]}_k}{\\left( \\sum_{j=1}^{C} z^{[L]}_j \\right)^2} $$\nSimplifying:\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = -\\frac{e^{z^{[L]}_i}}{\\sum^{C}_{j=1} e^{z^{[L]}_j}} \\cdot \\frac{e^{z^{[L]}_k}}{\\sum^{C}_{j=1} e^{z^{[L]}_j}} $$\nShortly, this is equivalent to:\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = - \\hat{y}_i \\cdot \\hat{y}_k $$\nFinally, we can summarize these two cases as\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = \\begin{cases} \\hat{y}_i \\cdot (1 - \\hat{y}_k) \u0026 \\text{ if } i = k \\\\ -\\hat{y}_i \\cdot \\hat{y}_k \u0026 \\text{ if } i \\neq k \\end{cases} $$\nIn literature, it is pretty common to use the following form:\n$$ \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_k} = \\hat{y}_i \\cdot (\\delta_{ik} - \\hat{y}_k) $$\nwhere $\\delta_{ik}$ is 1 if $i = k$, 0 otherwise. When we combine the gradient of cross-entropy and local gradient, we have:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial z^{[L]}_k} = \\sum_{i=1}^{C} -\\frac{y_i}{\\hat{y}_i} \\cdot \\hat{y}_i \\cdot (\\delta_{ik} - \\hat{y}_k) $$\nwhere $\\hat{y}_i$ cancels each other:\n$$ \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial z^{[L]}_k} \u0026 = \u0026 \\sum_{i=1}^{C} -y_i \\cdot (\\delta_{ik} - \\hat{y}_k) \\\\ \u0026 = \u0026 -y_k + \\hat{y}_k \\sum_{i=1}^C y_i \\end{array} $$\nSince ground truth value $y_i$ is one-hot vector (1 for true class and 0 for others), equation simplifies to:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial z^{[L]}_k} = \\hat{y}_k - y_k $$\nThis compact form is what makes softmax combined with cross-entropy loss so useful. Instead of dealing with complicated fractions, we can directly use this result to propagate gradients to the next layers.\nFrom this point, gradient calculations across layers will start to follow a recurring pattern. Each layer essentially repeats the same process: we compute gradients with respect to its inputs and its weights.\nFor the layer output $z^{[L]}$, we have two key variables to differentiate with respect to:\nThe weights $w^{[L]}$: their gradients are crucial because they are the trainable parameters we want to optimize during learning.\nThe activations $a^{[L-1]}$: while not parameters themselves, their gradients are equally important since they ensure the flow of gradients backward through the network, enabling earlier layers to update as well.\nThus, even though only the weight gradients directly influence optimization, the activation gradients play a important role in keeping backpropagation alive throughout the entire network:\nIf we calculate derivatives with respect to weights $w^{[l]}$: $$ \\frac{\\partial \\mathcal{L}}{\\partial w^{[l]}} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} \\cdot \\frac{\\partial \\left( a^{[l-1]} \\cdot w^{[l]} \\right)}{\\partial w^{[l]}} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} \\cdot a^{[l-1]} $$\nIf we calculate derivatives with respect to weights $a^{[l-1]}$: $$ \\frac{\\partial \\mathcal{L}}{\\partial a^{[l-1]}} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} \\cdot \\frac{\\partial \\left( a^{[l-1]} \\cdot w^{[l]} \\right)}{\\partial a^{[l-1]}} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} \\cdot w^{[l]} $$\nLastly, we need to calculate the derivative of $a^{[l]}$ with respect to $z^{[l]}$ in $\\sigma(z^{[l]})$, assuming $\\sigma(z^{[l]}) = \\text{ReLU}(z^{[l]})$:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} = \\frac{\\partial \\mathcal{L}}{\\partial a^{[l]}} \\cdot \\frac{\\partial a^{[l]}}{\\partial z^{[l]}} = \\frac{\\partial \\mathcal{L}}{\\partial a^{[l]}} \\cdot 1(z^{[l]} \u003e 0) $$\nwhere $1(z^{[l]} \u003e 0)$ outputs 1 if $(z^{[l]} \u003e 0)$, 0 otherwise as a indicator function.\nTherefore, we can summarize backpropagation for scalar values with following pattern:\n$$ \\begin{array}{rll} \\frac{\\partial \\mathcal{L}}{\\partial z^{[L]}} \u0026= \\hat{y}-y \\\\ \\\\ \\vdots \\\\ \\\\ \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} \u0026= \\frac{\\partial \\mathcal{L}}{\\partial a^{[l]}} \\cdot \\frac{\\partial a^{[l]}}{\\partial z^{[l]}} \u0026= \\frac{\\partial \\mathcal{L}}{\\partial a^{[l]}} \\cdot 1(z^{[l]} \u003e 0) \\\\ \\frac{\\partial \\mathcal{L}}{\\partial w^{[l]}} \u0026= \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} \\cdot \\frac{\\partial z^{[l]}}{\\partial w^{[l]}} \u0026= \\frac{\\partial \\mathcal{L}}{z^{[l]}} \\cdot a^{[l-1]} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial a^{[l-1]}} \u0026= \\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}} \\cdot \\frac{\\partial z^{[l]}}{\\partial a^{[l-1]}} \u0026= \\frac{\\partial \\mathcal{L}}{z^{[l]}} \\cdot w^{[l]} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial z^{[l-1]}} \u0026= \\frac{\\partial \\mathcal{L}}{\\partial a^{[l-1]}} \\cdot \\frac{\\partial a^{[l-1]}}{\\partial z^{[l-1]}} \u0026= \\frac{\\partial \\mathcal{L}}{\\partial a^{[l-1]}} \\cdot 1(z^{[l-1]} \u003e 0) \\\\ \\\\ \\vdots \\\\ \\\\ \\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}} \u0026= \\frac{\\partial \\mathcal{L}}{\\partial a^{[1]}} \\cdot \\frac{\\partial a^{[1]}}{\\partial z^{[1]}} \u0026= \\frac{\\partial \\mathcal{L}}{\\partial a^{[1]}} \\cdot 1(z^{[1]} \u003e 0) \\\\ \\frac{\\partial \\mathcal{L}}{\\partial w^{[1]}} \u0026= \\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}} \\cdot \\frac{\\partial z^{[1]}}{\\partial w^{[1]}} \u0026= \\frac{\\partial \\mathcal{L}}{z^{[1]}} \\cdot a^{[0]} \\\\ \\end{array} $$\nBackward Propagation After computing the loss through forward propagation, we need to update the model’s weights to minimize this loss. Backpropagation is the algorithm that computes the gradients of the loss function with respect to each parameter in the network. We’ll derive these gradients step by step, working backwards from the loss to the input layer.\nGradient of Cross-Entropy with Softmax Starting with loss function, we calculate relative gradients and go back step by step to calculate further gradients. Since the model has predictions with Softmax activation and the loss is calculated with cross-entropy, the combination of these methods has a nice property which simplifies the gradient:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}} = \\frac{1}{N} \\ \\underbrace{(\\hat{Y}-Y)}_{[N \\times C]} $$\nor in matrix representation:\n$$ \\footnotesize \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}} = \\frac{1}{N} \\ \\scriptsize \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{11}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{12}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{1C}} \\\\ \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{21}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{22}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{2C}} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{N1}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{N2}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}_{NC}} \\\\ \\end{bmatrix} \\footnotesize = \\frac{1}{N} \\ \\begin{bmatrix} \\hat{Y}_{11} - Y_{11} \u0026 \\hat{Y}_{12} - Y_{12} \u0026 \\dots \u0026 \\hat{Y}_{1C} - Y_{1C} \\\\ \\hat{Y}_{21} - Y_{21} \u0026 \\hat{Y}_{22} - Y_{22} \u0026 \\dots \u0026 \\hat{Y}_{2C} - Y_{2C} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\hat{Y}_{N1} - Y_{N1} \u0026 \\hat{Y}_{N2} - Y_{N2} \u0026 \\dots \u0026 \\hat{Y}_{NC} - Y_{NC} \\\\ \\end{bmatrix} $$\nor in visual representation:\nGradient of Arbitrary Layer $l$ At this stage, we can begin calculating the gradients of the weights in the corresponding layer. Since the forward propagation is computed as\n$$ Z^{[l]} = A^{[l-1]} W^{[l]} $$\nThere are two possible local gradients we might consider:\n$$ \\frac{\\partial Z^{[l]}}{\\partial W^{[l]}} \\quad \\text{ or } \\quad \\frac{\\partial Z^{[l]}}{\\partial A^{[l-1]}} $$\nYou may wonder why we would need the gradient with respect to $A^{[l-1]}$ since our goal is to update the weight matrix $W^{[l]}$. The reason is that the derivative with respect to $A^{[l-1]}$ becomes necessary for updating the weights of the previous layer, because\n$$ A^{[l-1]} = \\sigma\\left(A^{[l-2]} W^{[l-1]} \\right) $$\nWhen we want to calculate the gradients of weight matrix $W^{[l]}$ with respect to the loss function $\\mathcal{L}$, we use chain rule as follows:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{\\text{Upstream Gradient }} \\ \\cdot \\ \\underbrace{\\frac{\\partial Z^{[l]}}{\\partial W^{[l]}}}_{ \\text{ Local Gradient}} $$\nThe local gradient gives us:\n$$ \\frac{\\partial Z^{[l]}}{\\partial W^{[l]}} = \\frac{\\partial \\left( A^{[l-1]} W^{[l]} \\right)}{\\partial W^{[l]}} = \\underbrace{A^{[l-1]}}_{[N \\times H_{l-1}]} $$\nSince there are $H_{l-1} \\times H_l$ values in weight matrix $W^{[l]}$, we need to find each individual gradient so the shape of $\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}$ should be $H_{l-1} \\times H_l$. However, the dimensions of matrices does not match when we try to calculate matrix multiplication:\n$$ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}}_{[H_{l-1} \\times H_{l}]} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} \\quad \\underbrace{A^{[l-1]}}_{[N \\times H_{l-1}]} $$\nIf we take transpose of $A^{[l-1]}$ and place it to the left side of upstream gradient, we have the correct dimension matching:\n$$ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}}_{[H_{l-1} \\times H_{l}]} = \\underbrace{{A^{[l-1]}}^T}_{[H_{l-1} \\times N]} \\ \\cdot \\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} $$\nAt first glance, this transformation may seem arbitrary. Why do we transpose? How can we be sure this result is correct? To clarify, let’s check the matrix representation:\n$$ \\scriptsize \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{11}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{12}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{1H_{l-1}}} \\\\ \\\\ \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{21}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{22}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{2H_{l-1}}} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{H_{l}1}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{H_{l}2}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{H_{l}H_{l-1}}} \\\\ \\end{bmatrix} \\footnotesize = \\begin{bmatrix} {A^{[l-1]}_{11}}^T \u0026 {A^{[l-1]}_{12}}^T \u0026 \\dots \u0026 {A^{[l-1]}_{1N}}^T \\\\ {A^{[l-1]}_{21}}^T \u0026 {A^{[l-1]}_{22}}^T \u0026 \\dots \u0026 {A^{[l-1]}_{2N}}^T \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ {A^{[l-1]}_{H_{l-1}1}}^T \u0026 {A^{[l-1]}_{H_{l-1}2}}^T \u0026 \\dots \u0026 {A^{[l-1]}_{H_{l-1}N}}^T \\\\ \\end{bmatrix} \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{11}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{12}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{1H_{l}}} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{21}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{22}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{2H_{l}}} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{N1}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{N2}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{NH_{l}}} \\\\ \\end{bmatrix} $$\nIf we look closely only one gradient calculation, such as $\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{11}}$:\n$$ \\footnotesize \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{11}} = \\begin{bmatrix} {A^{[l-1]}_{11}}^T \u0026 {A^{[l-1]}_{12}}^T \u0026 \\dots \u0026 {A^{[l-1]}_{1N}}^T \\\\ \\end{bmatrix} \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{11}} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{21}} \\\\ \\vdots \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{N1}} \\\\ \\end{bmatrix} $$\nSince single value of weight matrix is fixed for each input value, the gradient of this weight is just summation of all $N$ input derivatives:\n$$ \\footnotesize \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{11}} = {A^{[l-1]}_{11}}^T \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{11}} + {A^{[l-1]}_{12}}^T \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{21}} + \\dots + {A^{[l-1]}_{1N}}^T \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{N1}} $$\nIn the image below, you can see the visualization of this calculation for 4 weights in the network:\nAt the same time, we need to calculate the derivative of $A^{[l-1]}$, $\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}$, so upstream gradients can flow to previous layer and the gradient of weight matrix can be calculated in that layer:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial A^{[l-1]}} $$\nThe local gradient is:\n$$ \\frac{\\partial Z^{[L]}}{\\partial A^{[l-1]}} = \\frac{\\partial \\left(A^{[l-1]} W^{[l]} \\right)}{\\partial A^{[l-1]}} = \\underbrace{W^{[l]}}_{[H_{l-1} \\times H_{l}]} $$\nOnce again, the dimensions of matrices do not match:\n$$ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}}_{[N \\times H_{l-1}]} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} \\ \\cdot \\ \\underbrace{W^{[l]}}_{[H_{l-1} \\times H_{l}]} $$\nWe can transpose the weight matrix to align matrices:\n$$ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}}_{[N \\times H_{l-1}]} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} \\quad \\underbrace{W^{[l]^T}}_{[H_{l} \\times H_{l-1}]} $$\nWe can see the reasoning more easily if we check matrix representation again:\n$$ \\footnotesize \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{11}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{12}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{1H_{l-1}}} \\\\ \\\\ \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{21}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{22}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{2H_{l-1}}} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{N1}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{N2}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{NH_{l-1}}} \\\\ \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{11}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{12}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{1H_{l}}} \\\\ \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{21}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{22}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{2H_{l}}} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{N1}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{N2}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{NH_{l}}} \\\\ \\end{bmatrix} \\begin{bmatrix} {W^{[l]^T}_{11}} \u0026 {W^{[l]^T}_{12}} \u0026 \\dots \u0026 {W^{[l]^T}_{1H_{l-1}}} \\\\ {W^{[l]^T}_{21}} \u0026 {W^{[l]^T}_{22}} \u0026 \\dots \u0026 {W^{[l]^T}_{2H_{l-1}}} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ {W^{[l]^T}_{H_{l}1}} \u0026 {W^{[l]^T}_{H_{l}2}} \u0026 \\dots \u0026 {W^{[l]^T}_{H_{l}H_{l-1}}} \\\\ \\end{bmatrix} $$\nBy focusing on only first value, $\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{11}}$, the gradient of $A^{l}_{11}$ depends on the values that it affected during forward propagation (the same logic as $\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}_{11}}$) :\n$$ \\footnotesize \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}_{11}} = \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{11}} \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{12}} \u0026 \\dots \u0026 \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}_{1H_{l}}} \\\\ \\end{bmatrix} \\begin{bmatrix} {W^{[l]^T}_{11}} \\\\ {W^{[l]^T}_{21}} \\\\ \\vdots \\\\ {W^{[l]^T}_{H_{l}1}} \\\\ \\end{bmatrix} $$\nNow we want to compute the gradient $\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l-1]}}$. Recall that $A^{[l-1]}$ is obtained by applying an activation function to $Z^{[l-1]}$:\n$$ \\underbrace{A^{[l-1]}}_{[N \\times H_{l-1}]} = \\underbrace{\\sigma(Z^{[l-1]})}_{[N \\times H_{l-1}]} $$\nIn our examples, we use the ReLU activation function. Its derivative with respect to each element is given by:\n$$ \\frac{\\partial \\ \\text{ReLU}(Z^{[l-1]}_{ij})}{\\partial Z^{[l-1]}_{ij}} = \\begin{cases} 1 \u0026 \\text{ if } \\ Z^{[l]}_{ij} \u003e 0, \\\\ 0 \u0026 \\text{ otherwise } \\end{cases} $$\nFor $N \\times H_{l-1}$ inputs, we must compute a gradient for each element, producing a matrix of 0s and 1s. The derivative of ReLU applied element-wise can be written as the indicator (mask) of positive entries:\n$$ \\frac{\\partial \\ \\text{ReLU}(Z^{[l-1]})}{\\partial Z^{[l-1]}} = 1\\left(Z^{[l-1]} \u003e 0\\right) $$\nwhere $1(\\cdot)$ is the element-wise indicator function (1 when the condition is true, 0 otherwise). In matrix form:\n$$ \\footnotesize \\frac{\\partial \\ \\text{ReLU}(Z^{[l-1]})}{\\partial Z^{[l-1]}} = \\begin{bmatrix} 1(Z^{[l-1]}_{11} \u003e 0) \u0026 1(Z^{[l-1]}_{12} \u003e 0) \u0026 \\dots \u0026 1(Z^{[l-1]}_{1H_{l-1}} \u003e 0) \\\\ 1(Z^{[l-1]}_{21} \u003e 0) \u0026 1(Z^{[l-1]}_{22} \u003e 0) \u0026 \\dots \u0026 1(Z^{[l-1]}_{2H_{l-1}} \u003e 0) \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ 1(Z^{[l-1]}_{N1} \u003e 0) \u0026 1(Z^{[l-1]}_{N2} \u003e 0) \u0026 \\dots \u0026 1(Z^{[l-1]}_{NH_{l-1}} \u003e 0) \\\\ \\end{bmatrix} $$\nBecause the derivative is applied element-wise, the upstream gradient $\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}$ is combined with this mask element-wise, not by matrix multiplication:\n$$ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l-1]}}}_{[N \\times H_{l-1}]} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}}_{[N \\times H_{l-1}]} \\odot \\underbrace{\\frac{\\partial A^{[l-1]}}{\\partial Z^{[l-1]}}}_{[N \\times H_{l-1}]} $$\nor\n$$ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l-1]}}}_{[N \\times H_{l-1}]} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}}_{[N \\times H_{l-1}]} \\odot \\underbrace{1\\left(Z^{[l-1]} \u003e 0\\right)}_{[N \\times H_{l-1}]} $$\nThe Pattern At this point, we completed the backpropagation using the matrix multiplication step. From here on, the process is essentially a repetition: each layer applies the same logic and propagates the gradients backwards through the weights (also biases implicitly) and activations.\nThe only real exception was the output layer, where we combined the softmax function with cross-entropy loss. This required a more detailed derivation, but the model is consistent for all hidden layers and is repeated across the entire network. You can see the pattern as follows:\n$$ \\footnotesize \\begin{array}{rll} \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}}}_{[N \\times C]} \u0026= \\frac{1}{N} \\underbrace{\\left(\\hat{Y} - Y\\right)}_{N \\times C} \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial W^{[L]}}}_{[H_{L-1} \\times C]} \u0026= \\underbrace{\\frac{\\partial \\left(A^{[L-1]} W^{[L]} \\right)}{\\partial W^{[L]}}}_{[H_{L-1} \\times N]} \\quad \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}}}_{[N \\times C]} \u0026= \\underbrace{A^{[L-1]^T}}_{[H_{L-1} \\times N]} \\quad \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}}}_{[N \\times C]} \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[L-1]}}}_{[N \\times H_{L-1}]} \u0026= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}}}_{[N \\times C]} \\quad \\underbrace{\\frac{\\partial \\left(A^{[L-1]} W^{[L]} \\right)}{\\partial A^{[L-1]}}}_{[C \\times H_{L-1}]} \u0026= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[L]}}}_{[N \\times C]} \\quad \\underbrace{W^{[L]^T}}_{[C \\times H_{L-1}]} \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[L-1]}}}_{[N \\times H_{L-1}]} \u0026= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[L-1]}}}_{[N \\times H_{L-1}]} \\odot \\underbrace{\\frac{\\partial \\left(\\sigma\\left(Z^{[L-1]}\\right) \\right)}{\\partial Z^{[L-1]}}}_{[N \\times H_{L-1}]} \u0026= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[L-1]}}}_{[N \\times H_{L-1}]} \\odot \\underbrace{1 \\left( Z^{[L-1]} \u003e 0 \\right)}_{[N \\times H_{L-1}]} \\\\ \\\\ \\vdots \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}}_{[H_{l-1} \\times H_{l}]} \u0026=\\underbrace{\\frac{\\partial \\left(A^{[l-1]} W^{[l]} \\right)}{\\partial W^{[l]}}}_{[H_{l-1} \\times N]} \\quad \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} \u0026= \\underbrace{A^{[l-1]^T}}_{[H_{l-1} \\times N]} \\quad \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}}_{[N \\times H_{l-1}]} \u0026= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} \\quad \\underbrace{\\frac{\\partial \\left(A^{[l-1]} W^{[l]} \\right)}{\\partial A^{[l-1]}}}_{[H_{l} \\times H_{l-1}]} \u0026= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}}_{[N \\times H_{l}]} \\quad \\underbrace{W^{[l]^T}}_{[H_{l} \\times H_{l-1}]} \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[l-1]}}}_{[N \\times H_{l-1}]} \u0026= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}}_{[N \\times H_{l-1}]} \\odot \\underbrace{\\frac{\\partial \\left(\\sigma\\left(Z^{[l-1]}\\right) \\right)}{\\partial Z^{[l-1]}}}_{[N \\times H_{l-1}]} \u0026= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}}}_{[N \\times H_{l-1}]} \\odot \\underbrace{1 \\left( Z^{[l-1]} \u003e 0 \\right)}_{[N \\times H_{l-1}]} \\\\ \\\\ \\vdots \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[1]}}}_{[N \\times H_{1}]} \u0026= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[1]}}}_{[N \\times H_{1}]} \\odot \\underbrace{\\frac{\\partial \\left(\\sigma\\left(Z^{[1]}\\right) \\right)}{\\partial Z^{[1]}}}_{[N \\times H_{1}]} \u0026= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial A^{[1]}}}_{[N \\times H_{1}]} \\odot \\underbrace{1 \\left( Z^{[1]} \u003e 0 \\right)}_{[N \\times H_{1}]} \\\\ \\\\ \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}}}_{[H_{0} \\times H_{1}]} \u0026= \\underbrace{\\frac{\\partial \\left(A^{[0]} W^{[1]} \\right)}{\\partial W^{[1]}}}_{[H_{0} \\times N]} \\quad \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[1]}}}_{[N \\times H_{1}]} \u0026= \\underbrace{A^{[0]^T}}_{[H_{0} \\times N]} \\quad \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial Z^{[1]}}}_{[N \\times H_{1}]} \\\\ \\end{array} $$\nI know it looks ugly but if you pay some attention, you will see the pattern for gradient calculation. There are just some rules that you need to follow for each block.\n",
  "wordCount" : "4815",
  "inLanguage": "en",
  "datePublished": "2025-09-18T12:05:30+03:00",
  "dateModified": "2025-09-18T12:05:30+03:00",
  "author":{
    "@type": "Person",
    "name": "Author: Umut Kaan Kavaklı"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://umutkavakli.github.io/posts/2025-09-18-backpropagation-through-matrix-multiplication/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "umut's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://umutkavakli.github.io/assets/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://umutkavakli.github.io/" accesskey="h" title="umut&#39;s blog (Alt + H)">
                    <img src="https://umutkavakli.github.io/images/ginger_hu_37674e613bbd587b.png" alt="" aria-label="logo"
                        height="30">umut&#39;s blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://umutkavakli.github.io/posts/" title="posts">
                    <span>posts</span>
                </a>
            </li>
            <li>
                <a href="https://umutkavakli.github.io/projects/" title="projects">
                    <span>projects</span>
                </a>
            </li>
            <li>
                <a href="https://umutkavakli.github.io/archives/" title="archives">
                    <span>archives</span>
                </a>
            </li>
            <li>
                <a href="https://umutkavakli.github.io/search/" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Backpropagation Through Matrix Multiplication
    </h1>
    <div class="post-meta"><span title='2025-09-18 12:05:30 +0300 +03'>September 18, 2025</span>&nbsp;·&nbsp;<span>23 min</span>&nbsp;·&nbsp;<span>Author: Umut Kaan Kavaklı</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#forward-propagation" aria-label="Forward Propagation">Forward Propagation</a><ul>
                        
                <li>
                    <a href="#adding-non-linearity" aria-label="Adding Non-Linearity">Adding Non-Linearity</a></li>
                <li>
                    <a href="#multi-class-classification" aria-label="Multi-class Classification">Multi-class Classification</a></li>
                <li>
                    <a href="#batch-processing" aria-label="Batch Processing">Batch Processing</a></li>
                <li>
                    <a href="#multi-layer-networks" aria-label="Multi-layer Networks">Multi-layer Networks</a></li>
                <li>
                    <a href="#softmax-for-classification" aria-label="Softmax for Classification">Softmax for Classification</a></li>
                <li>
                    <a href="#cross-entropy-loss-function" aria-label="Cross-Entropy Loss Function">Cross-Entropy Loss Function</a></li></ul>
                </li>
                <li>
                    <a href="#simple-computation-graph" aria-label="Simple Computation Graph">Simple Computation Graph</a><ul>
                        
                <li>
                    <a href="#chain-rule" aria-label="Chain Rule">Chain Rule</a></li>
                <li>
                    <a href="#scalar-backward-propagation" aria-label="Scalar Backward Propagation">Scalar Backward Propagation</a><ul>
                        
                <li>
                    <a href="#case-1-i--k" aria-label="Case 1: $i = k$">Case 1: $i = k$</a></li>
                <li>
                    <a href="#case-2-i-mathrelchar-k" aria-label="Case 2: $i \mathrel{\char`≠} k$">Case 2: $i \mathrel{\char`≠} k$</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#backward-propagation" aria-label="Backward Propagation">Backward Propagation</a><ul>
                        
                <li>
                    <a href="#gradient-of-cross-entropy-with-softmax" aria-label="Gradient of Cross-Entropy with Softmax">Gradient of Cross-Entropy with Softmax</a></li>
                <li>
                    <a href="#gradient-of-arbitrary-layer-l" aria-label="Gradient of Arbitrary Layer $l$">Gradient of Arbitrary Layer $l$</a></li>
                <li>
                    <a href="#the-pattern" aria-label="The Pattern">The Pattern</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><strong>TL;DR</strong> <br>
In this long and technical blog, I explained:</p>
<ul>
<li>
<p><strong>Forward propagation:</strong> How inputs flow through the network layer by layer (using matrix operations) to generate predictions.</p>
</li>
<li>
<p><strong>Computation Graph:</strong> How simple scalar examples help visualize backpropagation and build intuition before scaling up.</p>
</li>
<li>
<p><strong>Backward propagation:</strong> How errors flow backward (again with matrix operations) to compute gradients and update weights.</p>
</li>
</ul>
<hr>
<p>Backpropagation appears quite straightforward when working with scalars or even simple vectors. However, once we step into the world of matrices, things quickly become more complex and difficult to follow. There are extra details and notations that make it less intuitive.</p>
<p>Personally, although I managed to understand this concept while preparing for deep learning exams, I usually lose my intuition a few months later. Reviewing it again requires extra effort to bring everything back together. That&rsquo;s why I decided to write this technical blog post, both as a reminder for myself and as a guide for anyone trying to understand backpropagation in matrix form.</p>
<h2 id="forward-propagation">Forward Propagation<a hidden class="anchor" aria-hidden="true" href="#forward-propagation">#</a></h2>
<p>When training a deep learning model, the first step is to compute a linear combination of input features. Given an input vector $x \in \mathbb{R}^M$ with $M$ features and a corresponding weight vector $w \in \mathbb{R}^M$, we calculate a weighted sum and add a bias term $b \in \mathbb{R}^1$ to produce a single output $z$:</p>
<p>$$
z = w_1 \cdot x_1 + w_2 \cdot x_2 + \dots + w_M \cdot x_M + b
$$</p>
<p>This is simply the dot product between $w$ and $x$ with an added bias term:</p>
<p>$$
z = w^T x + b
$$</p>
<p>To visualize this computation, we can represent it in matrix form:</p>
<p>$$
\begin{bmatrix}
z
\end{bmatrix}
=
\begin{bmatrix}
w_1 &amp; w_2 &amp; \cdots &amp; w_M
\end{bmatrix}
\cdot
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_M
\end{bmatrix}
+
\begin{bmatrix}
b
\end{bmatrix}
$$</p>
<p>or in image representation:</p>
<figure class="align-center ">
    <img loading="lazy" src="images/net1.png#center" width="50%"/> 
</figure>

<p>We can simplify our notation by incorporating the bias term directly into the weight vector. This involves adding the bias $b$ as the first element of the weight vector and including a constant input $x_0 = 1$:</p>
<p>$$
\begin{bmatrix}
z
\end{bmatrix}
=
\begin{bmatrix}
b &amp; w_1 &amp; w_2 &amp; \cdots &amp; w_M
\end{bmatrix}
\cdot
\begin{bmatrix}
1 \\
x_1 \\
x_2 \\
\vdots \\
x_M
\end{bmatrix}
$$</p>
<p>For notational clarity, we&rsquo;ll denote the bias term as $w_0 = b$ and the constant input as $x_0 = 1$. Under this convention, both vectors have dimension $M+1$: $w \in \mathbb{R}^{M+1}$ and $x \in \mathbb{R}^{M+1}$. Our equation becomes:</p>
<p>$$
z = w^T x
$$</p>
<p>or in matrix form:</p>
<p>$$
\begin{bmatrix}
z
\end{bmatrix}
=
\begin{bmatrix}
w_0 &amp; w_1 &amp; w_2 &amp; \cdots &amp; w_M
\end{bmatrix}
\cdot
\begin{bmatrix}
x_0 \\
x_1 \\
x_2 \\
\vdots \\
x_M
\end{bmatrix}
$$</p>
<p>or in image representation:</p>
<figure class="align-center ">
    <img loading="lazy" src="images/net2.png#center" width="50%"/> 
</figure>

<h3 id="adding-non-linearity">Adding Non-Linearity<a hidden class="anchor" aria-hidden="true" href="#adding-non-linearity">#</a></h3>
<p>The linear combination alone is insufficient for learning complex patterns. To add non-linearity, we apply an <strong>activation function</strong> $\sigma(\cdot)$ to the output $z$.</p>
<p>$$
a =  \sigma(z) =  \sigma(w^Tx)
$$</p>
<p>Although $\sigma$ often denotes the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid</a> function, here it represents a general activation function. For this explanation, we&rsquo;ll use the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> activation function in hidden layers due to its simplicity, computational efficiency, resistance to vanishing gradients, and widespread popularity:</p>
<p>$$
a = \sigma(z) = \max(0, z)
$$</p>
<p>we can show this with visual representation:</p>
<figure class="align-center ">
    <img loading="lazy" src="images/net3.png#center" width="50%"/> 
</figure>

<h3 id="multi-class-classification">Multi-class Classification<a hidden class="anchor" aria-hidden="true" href="#multi-class-classification">#</a></h3>
<p>In practice, deep learning models often solve multi-class problems where we need to predict one of $C$ possible classes. This requires the output $a$ to be $C$-dimensional vector, with each dimension representing the score for a particular class. The predicted class corresponds to the entry with the highest activation value.</p>
<p>To generate $C$ outputs, we need $C$ distinct weight vectors, each of dimension $(M+1)$. To achieve this, we need a separate weight vector for each class. Stacking them gives a weight matrix $W \in \mathbb{R}^{C \times (M+1)}$:</p>
<p>$$
\underbrace{a}_{[C \times 1]} = \sigma \left( \underbrace{W}_{[C \times (M+1)]} \quad \underbrace{x}_{[(M+1) \times 1]} \right)
$$</p>
<p>or in expanded form:</p>
<p>$$
\begin{bmatrix}
a_{1} \\
a_{2} \\
\vdots \\
a_{C}
\end{bmatrix}
=
\sigma \left(
\begin{bmatrix}
W_{10} &amp; W_{11} &amp; W_{12} &amp; \dots &amp; W_{1M} \\
W_{20} &amp; W_{21} &amp; W_{22} &amp; \dots &amp; W_{2M} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
W_{C0} &amp; W_{C1} &amp; W_{C2} &amp; \dots &amp; W_{CM}
\end{bmatrix}
\begin{bmatrix}
x_{0} \\
x_{1}  \\
x_{2} \\
\vdots \\
x_{M}
\end{bmatrix}
\right)
$$</p>
<p>or in visual form:</p>
<figure class="align-center ">
    <img loading="lazy" src="images/net4.png#center"/> 
</figure>

<p>Each figure represents the same network, but highlights different output paths from the same inputs, and each path is computed by its own set of output weights.</p>
<h3 id="batch-processing">Batch Processing<a hidden class="anchor" aria-hidden="true" href="#batch-processing">#</a></h3>
<p>The computations described so far process only a single input example (batch size = 1). In practice, we process multiple inputs in parallel to improve computational efficiency.</p>
<p>If we process batch size of $N$ inputs simultaneously, then our input becomes $X \in \mathbb{R}^{N \times (M+1)}$ (note the uppercase $X$ since we now have a matrix rather than a vector). To handle $N$ outputs while maintaining proper matrix dimensions, we transpose our weight matrix to $W \in \mathbb{R}^{(M+1) \times C}$:</p>
<p>$$
\underbrace{A}_{[N \times C]} = \sigma\left(\underbrace{X}_{[N \times (M+1)]} \quad \underbrace{W}_{[(M+1) \times C]}\right)
$$</p>
<p>or in matrix form:</p>
<p>$$
\footnotesize
\begin{bmatrix}
A_{11} &amp; A_{12} &amp; \dots &amp; A_{1C} \\
A_{21} &amp; A_{22} &amp; \dots &amp; A_{2C} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
A_{N1} &amp; A_{N2} &amp; \dots &amp; A_{NC} \\
\end{bmatrix}
=
\sigma\left(
\begin{bmatrix}
X_{10} &amp; X_{11} &amp; \dots &amp; X_{1M} \\
X_{20} &amp; X_{21} &amp; \dots &amp; X_{2M} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
X_{N0} &amp; X_{N1} &amp; \dots &amp; X_{NM} \\
\end{bmatrix}
\begin{bmatrix}
W_{01} &amp; W_{02} &amp; \dots &amp; W_{0C} \\
W_{11} &amp; W_{12} &amp; \dots &amp; W_{1C} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
W_{M1} &amp; W_{M2} &amp; \dots &amp; W_{MC}
\end{bmatrix}
\right)
$$</p>
<p>When dealing with batches, it can sometimes be hard to picture how forward propagation works. We can visualize the process like this:</p>
<figure class="align-center ">
    <img loading="lazy" src="images/net5.png#center" width="60%"/> 
</figure>

<p>Note that in this illustration, the non-linearity (activation function) is not explicitly shown. The final output $A$ actually corresponds to the activated values.</p>
<h3 id="multi-layer-networks">Multi-layer Networks<a hidden class="anchor" aria-hidden="true" href="#multi-layer-networks">#</a></h3>
<p>The calculations presented so far describe a single layer. Deep learning models stack multiple layers to learn increasingly complex representations. We can denote each layer with a superscript $[l]$ to represent the layer number:</p>
<p>$$
\begin{align*}
\underbrace{Z^{[l]}}_{[N \times H_{l}]} &amp;= \underbrace{A^{[l-1]}}_{[N \times H_{l-1}]} \quad \underbrace{W^{[l]}}_{[H_{l-1} \times H_{l}]} \\
\underbrace{A^{[l]}}_{[N \times H_{l}]} &amp;=  \underbrace{\sigma(Z^{[l]})}_{[N \times H_{l}]}
\end{align*}
$$</p>
<p>Here $H_l$ represents the number of hidden units in layer $l$. The input to layer $l$ activated output from the previous layer $l-1$. For simplicity, we usually define $A^{[0]} = X$ (the input layer).</p>
<figure class="align-center ">
    <img loading="lazy" src="images/net6.png#center"/> 
</figure>

<h3 id="softmax-for-classification">Softmax for Classification<a hidden class="anchor" aria-hidden="true" href="#softmax-for-classification">#</a></h3>
<p>For classification problems, we apply the <strong>softmax</strong> activation function to the final layer&rsquo;s output to convert the raw scores into a probability distribution. The softmax function ensures that all outputs sum to 1, allowing us to interpret them as class probabilities.</p>
<p>The softmax function is applied row-wise (for each input example) across the $C$ classes:</p>
<p>$$
\hat{Y}_{ij} = \text{softmax}(Z_{ij}) = \frac{e^{Z_{ij}}}{\sum_{l=1}^{C} e^{Z_{il}}} \quad\quad i = 1,2,\dots,N \quad \text{ and } \quad j =1,2,\dots,C
$$</p>
<p>Intuitively, for input example $i$, this computes the probability of class $j$ by normalizing the exponential of its score by the sum of exponentials across all $C$ possible classes.</p>
<p>The matrix representation shows this row-wise operation:</p>
<p>$$
\footnotesize
\begin{bmatrix}
\hat{Y}_{11} &amp; \hat{Y}_{12} &amp; \dots &amp; \hat{Y}_{1C} \\
\hat{Y}_{21} &amp; \hat{Y}_{22} &amp; \dots &amp; \hat{Y}_{2C} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\hat{Y}_{N1} &amp; \hat{Y}_{N2} &amp; \dots &amp; \hat{Y}_{NC} \\
\end{bmatrix}
=
\tiny
\begin{bmatrix}
\text{softmax}\big( &amp; Z_{11} &amp; Z_{12} &amp; \dots &amp; Z_{1C} &amp; \big) \\
\text{softmax}\big( &amp; Z_{21} &amp; Z_{22} &amp; \dots &amp; Z_{2C} &amp; \big) \\
&amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\text{softmax}\big( &amp; Z_{N1} &amp; Z_{N2} &amp; \dots &amp; Z_{NC} &amp; \big) \\
\end{bmatrix}
\footnotesize
=
\tiny
\begin{bmatrix}
\frac{e^{Z_{11}}}{\sum_{l=1}^{C} e^{Z_{1l}}} &amp; \frac{e^{Z_{12}}}{\sum_{l=1}^{C} e^{Z_{1l}}} &amp; \dots &amp; \frac{e^{Z_{1C}}}{\sum_{l=1}^{C} e^{Z_{1l}}} \\
&amp; &amp; &amp; \\
\frac{e^{Z_{21}}}{\sum_{l=1}^{C} e^{Z_{2l}}} &amp; \frac{e^{Z_{22}}}{\sum_{l=1}^{C} e^{Z_{2l}}} &amp; \dots &amp; \frac{e^{Z_{2C}}}{\sum_{l=1}^{C} e^{Z_{2l}}} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
&amp; &amp; &amp; \\
\frac{e^{Z_{N1}}}{\sum_{l=1}^{C} e^{Z_{Nl}}} &amp; \frac{e^{Z_{N2}}}{\sum_{l=1}^{C} e^{Z_{Nl}}} &amp; \dots &amp; \frac{e^{Z_{NC}}}{\sum_{l=1}^{C} e^{Z_{Nl}}} \\
\end{bmatrix}
$$</p>
<p>If we denote the last layer as $L$, then the model’s output for $N$ inputs and $C$ classes can be represented as follows:</p>
<figure class="align-center ">
    <img loading="lazy" src="images/net7.png#center"/> 
</figure>

<h3 id="cross-entropy-loss-function">Cross-Entropy Loss Function<a hidden class="anchor" aria-hidden="true" href="#cross-entropy-loss-function">#</a></h3>
<p>After obtaining the predicted probabilities $\hat{Y}$, we measure the model&rsquo;s performance by comparing these predictions with the ground truth labels $Y \in \mathbb{R}^{N \times C}$ using a <strong>loss function</strong>. The ground truth is represented as one-hot encoded vectors, where each input example has exactly one correct class.
For multi-class classification, we typically use <strong>Cross-Entropy (CE)</strong> loss:</p>
<p>$$
\mathcal{L} = \text{CE}(Y, \hat{Y}) = -\frac{1}{N} \sum_{i=1}^N \sum_{j=1}^C  Y_{ij} \log\hat{Y}_{ij}
$$</p>
<p>This formula computes the element-wise product between the true labels $Y$ and the logarithm of predicted probabilities $\log\hat{Y}$ then averages across all examples to produce a single scalar loss value.</p>
<p>We can express this using matrix operations with the element-wise (Hadamard) product $\odot$:</p>
<p>$$
\underbrace{\mathcal{L}}_{[1 \times 1]} = \text{CE}(Y, \hat{Y}) = -\frac{1}{N} \quad \underbrace{1^{T}}_{[1 \times N]} \quad  (\underbrace{Y \odot \log\hat{Y}}_{[N \times C]}) \quad \underbrace{1}_{[C \times 1]}
$$</p>
<p>Expanding this matrix operation:</p>
<p>$$
\mathcal{L} =
\footnotesize
-\frac{1}{N}
\begin{bmatrix}
1_{1} &amp; 1_{2} &amp; \dots &amp; 1_{N}
\end{bmatrix}
\left(\begin{bmatrix}
Y_{11} &amp; Y_{12} &amp; \dots &amp; Y_{1C} \\
Y_{21} &amp; Y_{22} &amp; \dots &amp; Y_{2C} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
Y_{N1} &amp; Y_{N2}&amp; \dots &amp; Y_{NC} \\
\end{bmatrix}
\odot
\begin{bmatrix}
\log\hat{Y}_{11} &amp; \log\hat{Y}_{12} &amp; \dots &amp; \log\hat{Y}_{1C} \\
\log\hat{Y}_{21} &amp; \log\hat{Y}_{22} &amp; \dots &amp; \log\hat{Y}_{2C} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\log\hat{Y}_{N1} &amp; \log\hat{Y}_{N2} &amp; \dots &amp; \log\hat{Y}_{NC} \\
\end{bmatrix}\right)
\begin{bmatrix}
1_{1} \\
1_{2} \\
\vdots \\
1_{C}
\end{bmatrix}
$$</p>
<p>We can simplify this by recognizing that multiplying by vectors of ones simply sums all elements in the matrix. Therefore:</p>
<p>$$
\mathcal{L} =
\small
-\frac{1}{N} \
\text{sum}\left(\begin{bmatrix}
Y_{11}\log\hat{Y}_{11} &amp; Y_{12}\log\hat{Y}_{12} &amp; \dots &amp; Y_{1C}\log\hat{Y}_{1C} \\
Y_{21}\log\hat{Y}_{21} &amp; Y_{22}\log\hat{Y}_{22} &amp; \dots &amp; Y_{2C}\log\hat{Y}_{2C} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
Y_{N1}\log\hat{Y}_{N1} &amp; Y_{N2}\log\hat{Y}_{N2} &amp; \dots &amp; Y_{NC}\log\hat{Y}_{NC} \\
\end{bmatrix}\right)
$$</p>
<p>We can show this summation with following image (excluding scaling factor $-\frac{1}{N}$):</p>
<figure class="align-center ">
    <img loading="lazy" src="images/net8.png#center"/> 
</figure>

<p>Although this is a general representation, we denote the output cross-entropy loss as $-\log\hat{Y}$. Since the ground truth $Y$ is one-hot vector, it is 1 for true class and 0 for others in each input. Therefore, the assumption in this case can be:</p>
<p>$$
\mathcal{L}_{ij} =
\begin{cases}
-\log\hat{Y}_{ij} &amp; \text{if } \ Y_{ij} = 1 \\
0 &amp; \text{otherwise}
\end{cases}
\quad\quad\text{for } \ i=1,2,\dots, N
$$</p>
<p>This completes our mathematical framework for forward propagation in deep learning models, from single predictions to batch processing with multi-class classification and loss computation.</p>
<h2 id="simple-computation-graph">Simple Computation Graph<a hidden class="anchor" aria-hidden="true" href="#simple-computation-graph">#</a></h2>
<p>Before diving lots of matrix gradient calculation, I would like to show a computation graph for forward and backward propagation so we can see the cases we need to be careful when we compute gradients. In this case, I want to only show scalar calculations first instead of thinking about matrices so we can adapt this into matrix backpropagation.</p>
<p>$$
\begin{array}{rcl}
a^{[0]} &amp; = &amp; x \\
\\
z^{[1]} &amp; = &amp; a^{[0]} \cdot w^{[1]} &amp; \\
a^{[1]} &amp; = &amp;  \sigma(z^{[1]}) \\
\\
z^{[2]} &amp; = &amp; a^{[1]} \cdot w^{[2]} &amp; \\
a^{[2]} &amp; = &amp;  \sigma(z^{[2]}) \\
\\
\vdots \\
\\
z^{[l]} &amp; = &amp; a^{[l-1]} \cdot w^{[l]} &amp; \\
a^{[l]} &amp; = &amp;  \sigma(z^{[l]}) \\
\\
\vdots \\
\\
z^{[L-1]} &amp; = &amp; a^{[L-2]} \cdot w^{[L-1]} &amp; \\
a^{[L-1]} &amp; = &amp; \sigma(z^{[L-1]}) \\
\\
z^{[L]} &amp; = &amp; a^{[L-1]} \cdot w^{[L]} &amp; \\
\hat{y} &amp; = &amp;  \text{softmax}(z^{[L]}) \\
\mathcal{L} &amp; = &amp; \text{CE}(\hat{y}) &amp; = -\sum^{C}_{i=1} y_i \log \hat{y}_i
\end{array}
$$</p>
<p>where $[l]$ and $[L]$ represents arbitrary layer $l$ and last layer $L$, respectively. $\sigma(\cdot)$ is activation function (ReLU in this case). In this representation, everything is scalar except $w^{[L]}$ because the output must be multi-class for softmax so I just played in the last layer to show a meaningful example.</p>
<p>We can visualize the computation graph of this network as follows:</p>
<figure class="align-center ">
    <img loading="lazy" src="images/net9.png#center"/> 
</figure>

<h3 id="chain-rule">Chain Rule<a hidden class="anchor" aria-hidden="true" href="#chain-rule">#</a></h3>
<p>Backpropagation relies on the chain rule of calculus to compute gradients efficiently. For a composite function $f(g(h(x)))$, the chain rule states:</p>
<p>$$
\frac{\partial f}{\partial x} = \frac{\partial f}{\partial g} \cdot \frac{\partial g}{\partial h} \cdot \frac{\partial h}{\partial x}
$$</p>
<p>In neural networks, we apply this principle to decompose the loss gradient into a product of simpler derivatives based on any parameter.</p>
<h3 id="scalar-backward-propagation">Scalar Backward Propagation<a hidden class="anchor" aria-hidden="true" href="#scalar-backward-propagation">#</a></h3>
<p>After computing loss, we can start computing gradients with respect to it. Since there are $C$ classes, we need to calculate derivative of each prediction $\hat{y}_i$:</p>
<p>$$
\frac{\partial \mathcal{L}}{\partial \hat{y}_i} = \frac{\partial \left( -y_i \log\hat{y}_i \right)}{\partial \hat{y}_i}  = -\frac{y_i}{\hat{y}_i}
$$</p>
<p>We can visualize this gradient flow like this:</p>
<figure class="align-center ">
    <img loading="lazy" src="images/net10.png#center" width="70%"/> 
</figure>

<p>Since the predicted output $\hat{y}_i$ is computed using $z^{[L]}_i$ in both the numerator and the denominator</p>
<p>$$
y_i = \text{softmax}(z^{[L]}_i) = \frac{e^{z^{[L]}_i}}{\sum^{C}_{j=1} e^{z^{[L]}_j}},
$$</p>
<p>the derivative $\frac{\partial \mathcal{L}}{\partial z^{[L]}_k}$ depends not only on the numerator but also on the <strong>denominator</strong>:</p>
<p>$$
\frac{\partial \mathcal{L}}{\partial z^{[L]}_k} = \sum_{i=1}^{C} \frac{\partial \mathcal{L}}{\partial \hat{y}_i} \cdot \frac{\partial \hat{y}_i}{\partial z^{[L]}_k}
$$</p>
<p>It might be easier to understand when you see visual representation:</p>
<figure class="align-center ">
    <img loading="lazy" src="images/net11.png#center" width="80%"/> 
</figure>

<p>Since we already know $\frac{\partial \mathcal{L}}{\partial \hat{y}_i}$, the gradient expression simplifies to:</p>
<p>$$
\frac{\partial \mathcal{L}}{\partial z^{[L]}_k} = \sum_{i=1}^{C} -\frac{y_i}{\hat{y}_i} \cdot \frac{\partial \hat{y_i}}{\partial z^{[L]}_k}
$$</p>
<p>The most confusing part begins here, because we now need to carefully compute the local gradient $\frac{\partial \hat{y}_i}{\partial z^{[L]}_k}$. To do this, we apply the quotient rule:</p>
<p>$$
\frac{\partial \hat{y}_i}{\partial z^{[L]}_k} = \frac{\partial \left(\frac{e^{z^{[L]}_i}}{\sum_{j=1}^C e^{z^{[L]}_j}} \right)}{\partial z^{[L]}_k} = \frac{\left(\frac{\partial e^{z^{[L]}_i}}{\partial z^{[L]}_k}\right) \cdot \sum_{j=1}^C e^{z^{[L]}_j} - e^{z^{[L]}_i} \cdot \left( \frac{\sum_{j=1}^C e^{z^{[L]}_j} }{\partial z^{[L]}_k}\right)} {\left(\sum_{j=1}^C e^{z^{[L]}_j} \right)^2}
$$</p>
<p>For this derivative, there are two distinct cases to consider:</p>
<p>$$
i = k \quad \text{ or } \quad i \neq k
$$</p>
<h4 id="case-1-i--k">Case 1: $i = k$<a hidden class="anchor" aria-hidden="true" href="#case-1-i--k">#</a></h4>
<p>The derivative of numerator:</p>
<p>$$
\frac{\partial e^{z^{[L]}_i}}{\partial z^{[L]}_k} = e^{z^{[L]}_i}
$$</p>
<p>The derivative of the denominator is:</p>
<p>$$
\frac{\partial \left(\sum_{j=1}^C e^{z^{[L]}_j} \right)}{\partial z^{[L]}_k} = e^{z^{[L]}_k}
$$</p>
<p>Substituting these into the quotient rule:</p>
<p>$$
\frac{\partial \hat{y}_i}{\partial z^{[L]}_k} = \frac{z^{[L]}_i \cdot \sum_{j=1}^{C} z^{[L]}_j - z^{[L]}_i \cdot z^{[L]}_k}{\left( \sum_{j=1}^{C} z^{[L]}_j \right)^2}
$$</p>
<p>Factorizing into:</p>
<p>$$
\frac{\partial \hat{y}_i}{\partial z^{[L]}_k} = \frac{e^{z^{[L]}_i}}{\sum^{C}_{j=1} e^{z^{[L]}_j}} \cdot \left(1 - \frac{e^{z^{[L]}_k}}{\sum^{C}_{j=1} e^{z^{[L]}_j}} \right)
$$</p>
<ul>
<li>Shortly, this is equivalent to:</li>
</ul>
<p>$$
\frac{\partial \hat{y}_i}{\partial z^{[L]}_k} = \hat{y}_i \cdot (1 - \hat{y}_k)
$$</p>
<h4 id="case-2-i-mathrelchar-k">Case 2: $i \mathrel{\char`≠} k$<a hidden class="anchor" aria-hidden="true" href="#case-2-i-mathrelchar-k">#</a></h4>
<p>The derivative of the numerator is <strong>zero</strong>, because $e^{z_i}$ does not depend on $z_k$.</p>
<p>The derivative of the denominator is:</p>
<p>$$
\frac{\partial \left(\sum_{j=1}^C e^{z^{[L]}_j} \right)}{\partial z^{[L]}_k} = e^{z^{[L]}_k}
$$</p>
<p>Thus, we have:</p>
<p>$$
\frac{\partial \hat{y}_i}{\partial z^{[L]}_k} = \frac{0 \cdot \sum_{j=1}^{C} z^{[L]}_j - z^{[L]}_i \cdot z^{[L]}_k}{\left( \sum_{j=1}^{C} z^{[L]}_j \right)^2}
$$</p>
<p>Simplifying:</p>
<p>$$
\frac{\partial \hat{y}_i}{\partial z^{[L]}_k} =  -\frac{e^{z^{[L]}_i}}{\sum^{C}_{j=1} e^{z^{[L]}_j}} \cdot \frac{e^{z^{[L]}_k}}{\sum^{C}_{j=1} e^{z^{[L]}_j}}
$$</p>
<p>Shortly, this is equivalent to:</p>
<p>$$
\frac{\partial \hat{y}_i}{\partial z^{[L]}_k} = - \hat{y}_i \cdot  \hat{y}_k
$$</p>
<p>Finally, we can summarize these two cases as</p>
<p>$$
\frac{\partial \hat{y}_i}{\partial z^{[L]}_k} =
\begin{cases}
\hat{y}_i \cdot (1 - \hat{y}_k) &amp; \text{ if } i = k \\
-\hat{y}_i \cdot \hat{y}_k &amp; \text{ if } i \neq k
\end{cases}
$$</p>
<p>In literature, it is pretty common to use the following form:</p>
<p>$$
\frac{\partial \hat{y}_i}{\partial z^{[L]}_k} = \hat{y}_i \cdot (\delta_{ik} - \hat{y}_k)
$$</p>
<p>where $\delta_{ik}$ is 1 if $i = k$, 0 otherwise. When we combine the gradient of cross-entropy and local gradient, we have:</p>
<p>$$
\frac{\partial \mathcal{L}}{\partial z^{[L]}_k} = \sum_{i=1}^{C} -\frac{y_i}{\hat{y}_i} \cdot \hat{y}_i \cdot (\delta_{ik} - \hat{y}_k)
$$</p>
<p>where $\hat{y}_i$ cancels each other:</p>
<p>$$
\begin{array}{rcl}
\frac{\partial \mathcal{L}}{\partial z^{[L]}_k} &amp; = &amp; \sum_{i=1}^{C} -y_i \cdot (\delta_{ik} - \hat{y}_k) \\
&amp; = &amp; -y_k + \hat{y}_k \sum_{i=1}^C y_i
\end{array}
$$</p>
<p>Since ground truth value $y_i$ is one-hot vector (1 for true class and 0 for others), equation simplifies to:</p>
<p>$$
\frac{\partial \mathcal{L}}{\partial z^{[L]}_k} = \hat{y}_k - y_k
$$</p>
<p>This compact form is what makes softmax combined with cross-entropy loss so useful. Instead of dealing with complicated fractions, we can directly use this result to propagate gradients to the next layers.</p>
<p>From this point, gradient calculations across layers will start to follow a recurring pattern. Each layer essentially repeats the same process: we compute gradients with respect to its <strong>inputs</strong> and <strong>its weights</strong>.</p>
<p>For the layer output $z^{[L]}$, we have two key variables to differentiate with respect to:</p>
<ul>
<li>
<p><strong>The weights $w^{[L]}$:</strong> their gradients are crucial because they are the trainable parameters we want to optimize during learning.</p>
</li>
<li>
<p><strong>The activations $a^{[L-1]}$:</strong> while not parameters themselves, their gradients are equally important since they ensure the flow of gradients backward through the network, enabling earlier layers to update as well.</p>
</li>
</ul>
<p>Thus, even though only the weight gradients directly influence optimization, the activation gradients play a important role in keeping backpropagation alive throughout the entire network:</p>
<figure class="align-center ">
    <img loading="lazy" src="images/net12.png#center"/> 
</figure>

<ul>
<li>If we calculate derivatives with respect to weights $w^{[l]}$:</li>
</ul>
<p>$$
\frac{\partial \mathcal{L}}{\partial w^{[l]}} = \frac{\partial \mathcal{L}}{\partial z^{[l]}} \cdot \frac{\partial \left( a^{[l-1]} \cdot w^{[l]} \right)}{\partial w^{[l]}} = \frac{\partial \mathcal{L}}{\partial z^{[l]}} \cdot a^{[l-1]}
$$</p>
<ul>
<li>If we calculate derivatives with respect to weights $a^{[l-1]}$:</li>
</ul>
<p>$$
\frac{\partial \mathcal{L}}{\partial a^{[l-1]}} = \frac{\partial \mathcal{L}}{\partial z^{[l]}} \cdot \frac{\partial \left( a^{[l-1]} \cdot w^{[l]} \right)}{\partial a^{[l-1]}} = \frac{\partial \mathcal{L}}{\partial z^{[l]}} \cdot w^{[l]}
$$</p>
<p>Lastly, we need to calculate the derivative of $a^{[l]}$ with respect to $z^{[l]}$ in $\sigma(z^{[l]})$, assuming $\sigma(z^{[l]}) = \text{ReLU}(z^{[l]})$:</p>
<p>$$
\frac{\partial \mathcal{L}}{\partial z^{[l]}} = \frac{\partial \mathcal{L}}{\partial a^{[l]}} \cdot \frac{\partial a^{[l]}}{\partial z^{[l]}} = \frac{\partial \mathcal{L}}{\partial a^{[l]}} \cdot 1(z^{[l]} &gt; 0)
$$</p>
<p>where $1(z^{[l]} &gt; 0)$ outputs 1 if $(z^{[l]} &gt; 0)$, 0 otherwise as a indicator function.</p>
<p>Therefore, we can summarize backpropagation for scalar values with following pattern:</p>
<p>$$
\begin{array}{rll}
\frac{\partial \mathcal{L}}{\partial z^{[L]}} &amp;= \hat{y}-y \\
\\
\vdots \\
\\
\frac{\partial \mathcal{L}}{\partial z^{[l]}} &amp;= \frac{\partial \mathcal{L}}{\partial a^{[l]}} \cdot \frac{\partial a^{[l]}}{\partial z^{[l]}} &amp;= \frac{\partial \mathcal{L}}{\partial a^{[l]}} \cdot 1(z^{[l]} &gt; 0) \\
\frac{\partial \mathcal{L}}{\partial w^{[l]}} &amp;= \frac{\partial \mathcal{L}}{\partial z^{[l]}} \cdot \frac{\partial z^{[l]}}{\partial w^{[l]}} &amp;= \frac{\partial \mathcal{L}}{z^{[l]}} \cdot a^{[l-1]} \\
\frac{\partial \mathcal{L}}{\partial a^{[l-1]}} &amp;= \frac{\partial \mathcal{L}}{\partial z^{[l]}} \cdot \frac{\partial z^{[l]}}{\partial a^{[l-1]}} &amp;= \frac{\partial \mathcal{L}}{z^{[l]}} \cdot w^{[l]} \\
\frac{\partial \mathcal{L}}{\partial z^{[l-1]}} &amp;= \frac{\partial \mathcal{L}}{\partial a^{[l-1]}} \cdot \frac{\partial a^{[l-1]}}{\partial z^{[l-1]}} &amp;= \frac{\partial \mathcal{L}}{\partial a^{[l-1]}} \cdot 1(z^{[l-1]} &gt; 0) \\
\\
\vdots \\
\\
\frac{\partial \mathcal{L}}{\partial z^{[1]}} &amp;= \frac{\partial \mathcal{L}}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}} &amp;= \frac{\partial \mathcal{L}}{\partial a^{[1]}} \cdot 1(z^{[1]} &gt; 0) \\
\frac{\partial \mathcal{L}}{\partial w^{[1]}} &amp;= \frac{\partial \mathcal{L}}{\partial z^{[1]}} \cdot \frac{\partial z^{[1]}}{\partial w^{[1]}} &amp;= \frac{\partial \mathcal{L}}{z^{[1]}} \cdot a^{[0]} \\
\end{array}
$$</p>
<h2 id="backward-propagation">Backward Propagation<a hidden class="anchor" aria-hidden="true" href="#backward-propagation">#</a></h2>
<p>After computing the loss through forward propagation, we need to update the model&rsquo;s weights to minimize this loss. Backpropagation is the algorithm that computes the gradients of the loss function with respect to each parameter in the network. We&rsquo;ll derive these gradients step by step, working backwards from the loss to the input layer.</p>
<h3 id="gradient-of-cross-entropy-with-softmax">Gradient of Cross-Entropy with Softmax<a hidden class="anchor" aria-hidden="true" href="#gradient-of-cross-entropy-with-softmax">#</a></h3>
<p>Starting with loss function, we calculate relative gradients and go back step by step to calculate further gradients. Since the model has predictions with Softmax activation and the loss is calculated with cross-entropy, the combination of these methods has a nice property which simplifies the gradient:</p>
<p>$$
\frac{\partial \mathcal{L}}{\partial Z^{[L]}} = \frac{1}{N} \ \underbrace{(\hat{Y}-Y)}_{[N \times C]}
$$</p>
<p>or in matrix representation:</p>
<p>$$
\footnotesize
\frac{\partial \mathcal{L}}{\partial Z^{[L]}} =
\frac{1}{N} \
\scriptsize
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial Z^{[L]}_{11}} &amp; \frac{\partial \mathcal{L}}{\partial Z^{[L]}_{12}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial Z^{[L]}_{1C}} \\
\\
\frac{\partial \mathcal{L}}{\partial Z^{[L]}_{21}} &amp; \frac{\partial \mathcal{L}}{\partial Z^{[L]}_{22}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial Z^{[L]}_{2C}} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial \mathcal{L}}{\partial Z^{[L]}_{N1}} &amp; \frac{\partial \mathcal{L}}{\partial Z^{[L]}_{N2}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial Z^{[L]}_{NC}} \\
\end{bmatrix}
\footnotesize
=
\frac{1}{N} \
\begin{bmatrix}
\hat{Y}_{11} - Y_{11} &amp; \hat{Y}_{12} - Y_{12} &amp; \dots &amp; \hat{Y}_{1C}  - Y_{1C} \\
\hat{Y}_{21} - Y_{21} &amp; \hat{Y}_{22} - Y_{22} &amp; \dots &amp; \hat{Y}_{2C} - Y_{2C} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\hat{Y}_{N1} - Y_{N1} &amp; \hat{Y}_{N2} - Y_{N2} &amp; \dots &amp; \hat{Y}_{NC} - Y_{NC} \\
\end{bmatrix}
$$</p>
<p>or in visual representation:</p>
<figure class="align-center ">
    <img loading="lazy" src="images/net13.png#center" width="50%"/> 
</figure>

<h3 id="gradient-of-arbitrary-layer-l">Gradient of Arbitrary Layer $l$<a hidden class="anchor" aria-hidden="true" href="#gradient-of-arbitrary-layer-l">#</a></h3>
<p>At this stage, we can begin calculating the gradients of the weights in the corresponding layer. Since the forward propagation is computed as</p>
<p>$$
Z^{[l]} = A^{[l-1]} W^{[l]}
$$</p>
<p>There are two possible local gradients we might consider:</p>
<p>$$
\frac{\partial Z^{[l]}}{\partial W^{[l]}}
\quad \text{ or } \quad
\frac{\partial Z^{[l]}}{\partial A^{[l-1]}}
$$</p>
<p>You may wonder why we would need the gradient with respect to $A^{[l-1]}$ since our goal is to update the weight matrix $W^{[l]}$. The reason is that the derivative with respect to $A^{[l-1]}$ becomes necessary for updating the weights of the previous layer, because</p>
<p>$$
A^{[l-1]} = \sigma\left(A^{[l-2]} W^{[l-1]} \right)
$$</p>
<p>When we want to calculate the gradients of weight matrix $W^{[l]}$ with respect to the loss function $\mathcal{L}$, we use chain rule as follows:</p>
<p>$$
\frac{\partial \mathcal{L}}{\partial W^{[l]}} = \underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[l]}}}_{\text{Upstream Gradient }} \ \cdot \  \underbrace{\frac{\partial Z^{[l]}}{\partial W^{[l]}}}_{ \text{ Local Gradient}}
$$</p>
<p>The local gradient gives us:</p>
<p>$$
\frac{\partial Z^{[l]}}{\partial W^{[l]}} = \frac{\partial \left( A^{[l-1]} W^{[l]} \right)}{\partial W^{[l]}} = \underbrace{A^{[l-1]}}_{[N \times H_{l-1}]}
$$</p>
<p>Since there are $H_{l-1} \times H_l$ values in weight matrix $W^{[l]}$, we need to find each individual gradient so the shape of $\frac{\partial \mathcal{L}}{\partial W^{[l]}}$ should be $H_{l-1} \times H_l$. However, the dimensions of matrices does not match when we try to calculate matrix multiplication:</p>
<p>$$
\underbrace{\frac{\partial \mathcal{L}}{\partial W^{[l]}}}_{[H_{l-1} \times H_{l}]} = \underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[l]}}}_{[N \times H_{l}]} \quad \underbrace{A^{[l-1]}}_{[N \times H_{l-1}]}
$$</p>
<p>If we take transpose of $A^{[l-1]}$ and place it to the left side of upstream gradient, we have the correct dimension matching:</p>
<p>$$
\underbrace{\frac{\partial \mathcal{L}}{\partial W^{[l]}}}_{[H_{l-1} \times H_{l}]} = \underbrace{{A^{[l-1]}}^T}_{[H_{l-1} \times N]} \ \cdot \ \underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[l]}}}_{[N \times H_{l}]}
$$</p>
<p>At first glance, this transformation may seem arbitrary. Why do we transpose? How can we be sure this result is correct? To clarify, let’s check the matrix representation:</p>
<p>$$
\scriptsize
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial W^{[l]}_{11}} &amp; \frac{\partial \mathcal{L}}{\partial W^{[l]}_{12}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial W^{[l]}_{1H_{l-1}}} \\
\\
\frac{\partial \mathcal{L}}{\partial W^{[l]}_{21}} &amp; \frac{\partial \mathcal{L}}{\partial W^{[l]}_{22}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial W^{[l]}_{2H_{l-1}}} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial \mathcal{L}}{\partial W^{[l]}_{H_{l}1}} &amp; \frac{\partial \mathcal{L}}{\partial W^{[l]}_{H_{l}2}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial W^{[l]}_{H_{l}H_{l-1}}} \\
\end{bmatrix}
\footnotesize
=
\begin{bmatrix}
{A^{[l-1]}_{11}}^T &amp; {A^{[l-1]}_{12}}^T &amp; \dots &amp; {A^{[l-1]}_{1N}}^T \\
{A^{[l-1]}_{21}}^T &amp; {A^{[l-1]}_{22}}^T &amp; \dots &amp; {A^{[l-1]}_{2N}}^T \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
{A^{[l-1]}_{H_{l-1}1}}^T &amp; {A^{[l-1]}_{H_{l-1}2}}^T &amp; \dots &amp; {A^{[l-1]}_{H_{l-1}N}}^T \\
\end{bmatrix}
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial Z^{[l]}_{11}} &amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{12}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{1H_{l}}} \\
\frac{\partial \mathcal{L}}{\partial Z^{[l]}_{21}} &amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{22}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{2H_{l}}} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial \mathcal{L}}{\partial Z^{[l]}_{N1}} &amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{N2}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{NH_{l}}} \\
\end{bmatrix}
$$</p>
<p>If we look closely only one gradient calculation, such as $\frac{\partial \mathcal{L}}{\partial W^{[l]}_{11}}$:</p>
<p>$$
\footnotesize
\frac{\partial \mathcal{L}}{\partial W^{[l]}_{11}}
=
\begin{bmatrix}
{A^{[l-1]}_{11}}^T &amp; {A^{[l-1]}_{12}}^T &amp; \dots &amp; {A^{[l-1]}_{1N}}^T \\
\end{bmatrix}
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial Z^{[l]}_{11}} \\
\frac{\partial \mathcal{L}}{\partial Z^{[l]}_{21}} \\
\vdots \\
\frac{\partial \mathcal{L}}{\partial Z^{[l]}_{N1}} \\
\end{bmatrix}
$$</p>
<p>Since single value of weight matrix is fixed for each input value, the gradient of this weight is just summation of all $N$ input derivatives:</p>
<p>$$
\footnotesize
\frac{\partial \mathcal{L}}{\partial W^{[l]}_{11}} = {A^{[l-1]}_{11}}^T \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{11}} + {A^{[l-1]}_{12}}^T \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{21}} + \dots + {A^{[l-1]}_{1N}}^T \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{N1}}
$$</p>
<p>In the image below, you can see the visualization of this calculation for 4 weights in the network:</p>
<figure class="align-center ">
    <img loading="lazy" src="images/net14.png#center"/> 
</figure>

<p>At the same time, we need to calculate the derivative of $A^{[l-1]}$, $\frac{\partial \mathcal{L}}{\partial A^{[l-1]}}$, so upstream gradients can flow to previous layer and the gradient of weight matrix can be calculated in that layer:</p>
<p>$$
\frac{\partial \mathcal{L}}{\partial A^{[l-1]}} = \frac{\partial \mathcal{L}}{\partial Z^{[l]}} \frac{\partial Z^{[l]}}{\partial A^{[l-1]}}
$$</p>
<p>The local gradient is:</p>
<p>$$
\frac{\partial Z^{[L]}}{\partial A^{[l-1]}} = \frac{\partial \left(A^{[l-1]} W^{[l]} \right)}{\partial A^{[l-1]}} = \underbrace{W^{[l]}}_{[H_{l-1} \times H_{l}]}
$$</p>
<p>Once again, the dimensions of matrices do not match:</p>
<p>$$
\underbrace{\frac{\partial \mathcal{L}}{\partial A^{[l-1]}}}_{[N \times H_{l-1}]} = \underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[l]}}}_{[N \times H_{l}]} \ \cdot \ \underbrace{W^{[l]}}_{[H_{l-1} \times H_{l}]}
$$</p>
<p>We can transpose the weight matrix to align matrices:</p>
<p>$$
\underbrace{\frac{\partial \mathcal{L}}{\partial A^{[l-1]}}}_{[N \times H_{l-1}]} = \underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[l]}}}_{[N \times H_{l}]} \quad \underbrace{W^{[l]^T}}_{[H_{l} \times  H_{l-1}]}
$$</p>
<p>We can see the reasoning more easily if we check matrix representation again:</p>
<p>$$
\footnotesize
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial A^{[l-1]}_{11}} &amp; \frac{\partial \mathcal{L}}{\partial A^{[l-1]}_{12}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial A^{[l-1]}_{1H_{l-1}}} \\
\\
\frac{\partial \mathcal{L}}{\partial A^{[l-1]}_{21}} &amp; \frac{\partial \mathcal{L}}{\partial A^{[l-1]}_{22}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial A^{[l-1]}_{2H_{l-1}}} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial \mathcal{L}}{\partial A^{[l-1]}_{N1}} &amp; \frac{\partial \mathcal{L}}{\partial A^{[l-1]}_{N2}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial A^{[l-1]}_{NH_{l-1}}} \\
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial Z^{[l]}_{11}} &amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{12}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{1H_{l}}} \\
\\
\frac{\partial \mathcal{L}}{\partial Z^{[l]}_{21}} &amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{22}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{2H_{l}}} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial \mathcal{L}}{\partial Z^{[l]}_{N1}} &amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{N2}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{NH_{l}}} \\
\end{bmatrix}
\begin{bmatrix}
{W^{[l]^T}_{11}} &amp; {W^{[l]^T}_{12}} &amp; \dots &amp; {W^{[l]^T}_{1H_{l-1}}} \\
{W^{[l]^T}_{21}} &amp; {W^{[l]^T}_{22}} &amp; \dots &amp; {W^{[l]^T}_{2H_{l-1}}} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
{W^{[l]^T}_{H_{l}1}} &amp; {W^{[l]^T}_{H_{l}2}} &amp; \dots &amp; {W^{[l]^T}_{H_{l}H_{l-1}}} \\
\end{bmatrix}
$$</p>
<p>By focusing on only first value, $\frac{\partial \mathcal{L}}{\partial A^{[l-1]}_{11}}$, the gradient of $A^{l}_{11}$ depends on the values that it affected during forward propagation (the same logic as $\frac{\partial \mathcal{L}}{\partial W^{[l]}_{11}}$) :</p>
<p>$$
\footnotesize
\frac{\partial \mathcal{L}}{\partial A^{[l-1]}_{11}}
=
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial Z^{[l]}_{11}} &amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{12}} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial Z^{[l]}_{1H_{l}}} \\
\end{bmatrix}
\begin{bmatrix}
{W^{[l]^T}_{11}} \\
{W^{[l]^T}_{21}} \\
\vdots \\
{W^{[l]^T}_{H_{l}1}} \\
\end{bmatrix}
$$</p>
<p>Now we want to compute the gradient $\frac{\partial \mathcal{L}}{\partial Z^{[l-1]}}$. Recall that $A^{[l-1]}$ is obtained by applying an activation function to $Z^{[l-1]}$:</p>
<p>$$
\underbrace{A^{[l-1]}}_{[N \times H_{l-1}]} = \underbrace{\sigma(Z^{[l-1]})}_{[N \times H_{l-1}]}
$$</p>
<p>In our examples, we use the ReLU activation function. Its derivative with respect to <strong>each element</strong> is given by:</p>
<p>$$
\frac{\partial \ \text{ReLU}(Z^{[l-1]}_{ij})}{\partial Z^{[l-1]}_{ij}} =
\begin{cases}
1 &amp; \text{ if } \ Z^{[l]}_{ij} &gt; 0, \\
0 &amp; \text{ otherwise }
\end{cases}
$$</p>
<p>For $N \times H_{l-1}$ inputs, we must compute a gradient for each element, producing a matrix of 0s and 1s. The derivative of ReLU applied element-wise can be written as the indicator (mask) of positive entries:</p>
<p>$$
\frac{\partial \ \text{ReLU}(Z^{[l-1]})}{\partial Z^{[l-1]}} = 1\left(Z^{[l-1]} &gt; 0\right)
$$</p>
<p>where $1(\cdot)$ is the element-wise indicator function (1 when the condition is true, 0 otherwise). In matrix form:</p>
<p>$$
\footnotesize
\frac{\partial \ \text{ReLU}(Z^{[l-1]})}{\partial Z^{[l-1]}} =
\begin{bmatrix}
1(Z^{[l-1]}_{11} &gt; 0) &amp; 1(Z^{[l-1]}_{12} &gt; 0) &amp; \dots &amp; 1(Z^{[l-1]}_{1H_{l-1}} &gt; 0) \\
1(Z^{[l-1]}_{21} &gt; 0) &amp; 1(Z^{[l-1]}_{22} &gt; 0) &amp; \dots &amp; 1(Z^{[l-1]}_{2H_{l-1}} &gt; 0) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1(Z^{[l-1]}_{N1} &gt; 0) &amp; 1(Z^{[l-1]}_{N2} &gt; 0) &amp; \dots &amp; 1(Z^{[l-1]}_{NH_{l-1}} &gt; 0) \\
\end{bmatrix}
$$</p>
<p>Because the derivative is applied element-wise, the upstream gradient $\frac{\partial \mathcal{L}}{\partial A^{[l-1]}}$ is combined with this mask element-wise, not by matrix multiplication:</p>
<p>$$
\underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[l-1]}}}_{[N \times H_{l-1}]} = \underbrace{\frac{\partial \mathcal{L}}{\partial A^{[l-1]}}}_{[N \times H_{l-1}]} \odot \underbrace{\frac{\partial A^{[l-1]}}{\partial Z^{[l-1]}}}_{[N \times H_{l-1}]}
$$</p>
<p>or</p>
<p>$$
\underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[l-1]}}}_{[N \times H_{l-1}]} = \underbrace{\frac{\partial \mathcal{L}}{\partial A^{[l-1]}}}_{[N \times H_{l-1}]} \odot \underbrace{1\left(Z^{[l-1]} &gt; 0\right)}_{[N \times H_{l-1}]}
$$</p>
<h3 id="the-pattern">The Pattern<a hidden class="anchor" aria-hidden="true" href="#the-pattern">#</a></h3>
<p>At this point, we completed the backpropagation using the matrix multiplication step. From here on, the process is essentially a <strong>repetition</strong>: each layer applies the same logic and propagates the gradients backwards through the weights (also biases implicitly) and activations.</p>
<p>The only real exception was the output layer, where we combined the softmax function with cross-entropy loss. This required a more detailed derivation, but the model is consistent for all hidden layers and is repeated across the entire network. You can see the pattern as follows:</p>
<p>$$
\footnotesize
\begin{array}{rll}
\underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[L]}}}_{[N \times C]} &amp;= \frac{1}{N} \underbrace{\left(\hat{Y} - Y\right)}_{N \times C} \\
\\
\underbrace{\frac{\partial \mathcal{L}}{\partial W^{[L]}}}_{[H_{L-1} \times C]} &amp;= \underbrace{\frac{\partial \left(A^{[L-1]} W^{[L]} \right)}{\partial W^{[L]}}}_{[H_{L-1} \times N]} \quad \underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[L]}}}_{[N \times C]} &amp;= \underbrace{A^{[L-1]^T}}_{[H_{L-1} \times N]} \quad \underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[L]}}}_{[N \times C]} \\
\\
\underbrace{\frac{\partial \mathcal{L}}{\partial A^{[L-1]}}}_{[N \times H_{L-1}]} &amp;= \underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[L]}}}_{[N \times C]} \quad \underbrace{\frac{\partial \left(A^{[L-1]} W^{[L]} \right)}{\partial A^{[L-1]}}}_{[C \times H_{L-1}]} &amp;= \underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[L]}}}_{[N \times C]} \quad \underbrace{W^{[L]^T}}_{[C \times H_{L-1}]} \\
\\
\underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[L-1]}}}_{[N \times H_{L-1}]} &amp;= \underbrace{\frac{\partial \mathcal{L}}{\partial A^{[L-1]}}}_{[N \times H_{L-1}]} \odot \underbrace{\frac{\partial \left(\sigma\left(Z^{[L-1]}\right) \right)}{\partial Z^{[L-1]}}}_{[N \times H_{L-1}]} &amp;=  \underbrace{\frac{\partial \mathcal{L}}{\partial A^{[L-1]}}}_{[N \times H_{L-1}]} \odot \underbrace{1 \left( Z^{[L-1]} &gt; 0 \right)}_{[N \times H_{L-1}]} \\
\\
\vdots \\
\\
\underbrace{\frac{\partial \mathcal{L}}{\partial W^{[l]}}}_{[H_{l-1} \times H_{l}]} &amp;=\underbrace{\frac{\partial \left(A^{[l-1]} W^{[l]} \right)}{\partial W^{[l]}}}_{[H_{l-1} \times N]} \quad \underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[l]}}}_{[N \times H_{l}]} &amp;= \underbrace{A^{[l-1]^T}}_{[H_{l-1} \times N]} \quad \underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[l]}}}_{[N \times H_{l}]} \\
\\
\underbrace{\frac{\partial \mathcal{L}}{\partial A^{[l-1]}}}_{[N \times H_{l-1}]} &amp;= \underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[l]}}}_{[N \times H_{l}]} \quad \underbrace{\frac{\partial \left(A^{[l-1]} W^{[l]} \right)}{\partial A^{[l-1]}}}_{[H_{l} \times H_{l-1}]} &amp;= \underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[l]}}}_{[N \times H_{l}]} \quad \underbrace{W^{[l]^T}}_{[H_{l} \times H_{l-1}]} \\
\\
\underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[l-1]}}}_{[N \times H_{l-1}]} &amp;= \underbrace{\frac{\partial \mathcal{L}}{\partial A^{[l-1]}}}_{[N \times H_{l-1}]} \odot \underbrace{\frac{\partial \left(\sigma\left(Z^{[l-1]}\right) \right)}{\partial Z^{[l-1]}}}_{[N \times H_{l-1}]} &amp;= \underbrace{\frac{\partial \mathcal{L}}{\partial A^{[l-1]}}}_{[N \times H_{l-1}]} \odot \underbrace{1 \left( Z^{[l-1]} &gt; 0 \right)}_{[N \times H_{l-1}]} \\
\\
\vdots \\
\\
\underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[1]}}}_{[N \times H_{1}]} &amp;= \underbrace{\frac{\partial \mathcal{L}}{\partial A^{[1]}}}_{[N \times H_{1}]} \odot \underbrace{\frac{\partial \left(\sigma\left(Z^{[1]}\right) \right)}{\partial Z^{[1]}}}_{[N \times H_{1}]} &amp;= \underbrace{\frac{\partial \mathcal{L}}{\partial A^{[1]}}}_{[N \times H_{1}]} \odot \underbrace{1 \left( Z^{[1]} &gt; 0 \right)}_{[N \times H_{1}]} \\
\\
\underbrace{\frac{\partial \mathcal{L}}{\partial W^{[1]}}}_{[H_{0} \times H_{1}]} &amp;= \underbrace{\frac{\partial \left(A^{[0]} W^{[1]} \right)}{\partial W^{[1]}}}_{[H_{0} \times N]} \quad \underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[1]}}}_{[N \times H_{1}]} &amp;= \underbrace{A^{[0]^T}}_{[H_{0} \times N]} \quad \underbrace{\frac{\partial \mathcal{L}}{\partial Z^{[1]}}}_{[N \times H_{1}]} \\
\end{array}
$$</p>
<p>I know it looks ugly but if you pay some attention, you will see the pattern for gradient calculation. There are just some rules that you need to follow for each block.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Backpropagation Through Matrix Multiplication on x"
            href="https://x.com/intent/tweet/?text=Backpropagation%20Through%20Matrix%20Multiplication&amp;url=https%3a%2f%2fumutkavakli.github.io%2fposts%2f2025-09-18-backpropagation-through-matrix-multiplication%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Backpropagation Through Matrix Multiplication on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fumutkavakli.github.io%2fposts%2f2025-09-18-backpropagation-through-matrix-multiplication%2f&amp;title=Backpropagation%20Through%20Matrix%20Multiplication&amp;summary=Backpropagation%20Through%20Matrix%20Multiplication&amp;source=https%3a%2f%2fumutkavakli.github.io%2fposts%2f2025-09-18-backpropagation-through-matrix-multiplication%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Backpropagation Through Matrix Multiplication on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fumutkavakli.github.io%2fposts%2f2025-09-18-backpropagation-through-matrix-multiplication%2f&title=Backpropagation%20Through%20Matrix%20Multiplication">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Backpropagation Through Matrix Multiplication on whatsapp"
            href="https://api.whatsapp.com/send?text=Backpropagation%20Through%20Matrix%20Multiplication%20-%20https%3a%2f%2fumutkavakli.github.io%2fposts%2f2025-09-18-backpropagation-through-matrix-multiplication%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Backpropagation Through Matrix Multiplication on telegram"
            href="https://telegram.me/share/url?text=Backpropagation%20Through%20Matrix%20Multiplication&amp;url=https%3a%2f%2fumutkavakli.github.io%2fposts%2f2025-09-18-backpropagation-through-matrix-multiplication%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://umutkavakli.github.io/">umut&#39;s blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
